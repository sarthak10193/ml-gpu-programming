{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is splitK ?\n",
    "In a traditional GeMM, each threadblock is responsible for performing the reduction over **ENTIRE K dim**. When K is large this increases the work per threadblock. \n",
    "\n",
    "Instead of having 1 tile's output be computed by 1 threadblock and the reduction be done over entire K, **SplitK** will have multiple threablock  $TB_0$, $TB_1$, ... $TB_i$ compute the reduction over K and then use Atomics (atomic add). \n",
    "\n",
    "Each threadblock is responsible for computing the partial result over tile vs a complete tile. The partial results then are then reduced using **atomic_add**. Obvisouly as a by product of this, each threadblock has less work to do compared to what a threadblock would do in case of a standard data parallel tile, leading to better SM Utilization. \n",
    "\n",
    "## When may splitK be effective ?\n",
    "1. Memory bound GeMMs: Inference workloads often skinny GeMM\n",
    "2. Due to splitting along the K dim, each threadblock ....\n",
    "3. Reduce Wave quantization ...\n",
    "\n",
    "## \n",
    "1. Better loadbalaning for compute bound kernels or useful for memory bound kernels   \n",
    "\n",
    "## Trade offs\n",
    "* Note that there is a tension between the improvements from finer grained SM work distribution, and the overhead of thread blocks contending for exclusive write access to the same output buffer. This effect was seen on an A100 where increasing the SplitK parameter from 4 to 16, resulted in a steady degradation of performance as the matrix sizes increased, presumably due to greater wait times per thread block to get exclusive write access to the same memory output buffer.\n",
    "\n",
    "## Real world\n",
    "1. In an online inference workloads batch sizes are often small yeilding a skinny activation matrix. \n",
    "\n",
    "$C = \\beta W A  + C$\n",
    "\n",
    "\n",
    "Intuitively put, When multiplying two matrices (i.e. performing GEMM), each element of the output is computed as a sum over products along the “K” dimension. If that sum is very long, it can become a bottleneck. SplitK is a strategy that divides this long sum into several shorter, independent sums.\n",
    "\n",
    "## Readings\n",
    "1. https://github.com/pytorch-labs/applied-ai/tree/main/kernels\n",
    "2. https://arxiv.org/pdf/2402.00025\n",
    "3. https://pytorch.org/blog/accelerating-triton/\n",
    "4. https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/\n",
    "5. https://pytorch.org/blog/accelerating-moe-model/#30-work-decomposition---splitk\n",
    "6. https://github.com/pytorch-labs/gpt-fast/tree/main\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume a Batch size = 128\n",
    "\n",
    "* Input (A) = [128, 4096] # notice how this GeMM is skinny with 128 << 4096 \n",
    "* Weight (W) = [4096, 4096]\n",
    "* Output (C) = [128, 4096] \n",
    "\n",
    "So given these activation and weights matrices we have **M = 128**, **K = 4096**, and **N = 4096**\n",
    "\n",
    "* BLOCK_M = 16\n",
    "* BLOCK_N = 16\n",
    "\n",
    "Threfore each threadblock will process (128/16, 4096/16) = (8, 256) size outputs. **Grid size = 8*256 = 2048**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement triton (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for triton\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "import triton.lang as tl \n",
    "import torch \n",
    "import triton.testing as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_M = 128\n",
    "BLOCK_N = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "@triton.jit\n",
    "def splitK_kernel(\n",
    "    a,  # pointer to GPU location of matrix a\n",
    "    b,   # pointer to GPU location of matrix b\n",
    "    out, # pointer to GPU location of matrix out\n",
    "    stride_am, \n",
    "    stride_ak, \n",
    "    stride_bk, \n",
    "    stride_bn, \n",
    "    stride_om, \n",
    "    stride_on, \n",
    "    BLOCK_M: tl.constexpr, \n",
    "    BLOCK_N: tl.constexpr, \n",
    "    SPLIT_K: tl.constexpr\n",
    "):\n",
    "    # ProgramID is a instance of the program and is executed by a thread block. \n",
    "    pid_m = tl.progmram_id(0)\n",
    "    pid_n = tl.progmram_id(1)\n",
    "    pid_k = tl.progmram_id(2)\n",
    "\n",
    "    # Starting Indices for each block along m and n. \n",
    "    # assume (pid_m, pid_n) = 2, 3 then off_am = 2 * 128 + [0, 1, 2, .... 127]\n",
    "    # off_am = [256, 257, 258, 259, ..., 383]\n",
    "    # off_bn = [384, 385, 386, ..., 511]\n",
    "    # Therefore, this TB processes 128X128 tile dictated by off_am and off_bn. \n",
    "    off_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M) # ROW indices for A and C.\n",
    "    off_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N) # COL indices for B and C. \n",
    "\n",
    "    # accumulator for storing the partial results of this threadblock. \n",
    "    acc = tl.zeroes((BLOCK_M, BLOCK_N))\n",
    "\n",
    "    num_slices_K = tl.cdiv(k, SPLIT_K)\n",
    "\n",
    "    for k in range(start=pid_k, stop=num_slices_K, step=SPLIT_K):\n",
    "        off_k = k * SPLIT_K + tl.arange(SPLIT_K)\n",
    "        \n",
    "        # load a block from Matrix A, considering proper boundary conditions to prevent accessing \n",
    "        # wrong address spaces. \n",
    "        a_ptr = a + off_am[:, None] * stride_am + off_k[:, None] * stride_ak # list of contiguous addresses to read\n",
    "        a = tl.load(a_ptr, mask=(off_am[:, None] < BLOCK_M) & (off_k[None, :] < SPLIT_K), other=0.0)\n",
    "\n",
    "        b_ptr = b + off_k[:, None] * stride_bk + off_bn[None, :] * stride_bn\n",
    "        b = tl.load(b_ptr, mask=(off_k[:, None] < SPLIT_K) & (off_bn[None, :] < BLOCK_N), other=0.0)\n",
    "\n",
    "        acc += tl.dot(a, b)\n",
    "\n",
    "    out_ptr = out + pid_m * BLOCK_M * stride_om + pid_n * BLOCK_N * stride_on\n",
    "\n",
    "    tl.atomic_add(out_ptr, acc)\n",
    "\n",
    "\n",
    "def splitk_gemm(\n",
    "        A: torch.tensor,\n",
    "        B: torch.tensor, \n",
    "        splitK: int\n",
    "):\n",
    "    M, K = A.shape[0], A.shape[1]\n",
    "    N = B.shape[1]\n",
    "    out = torch.empty((M, N), dtype=torch.float16, device=A.device)\n",
    "\n",
    "    # ------ asserts -------------\n",
    "    assert A.size() == B.size() == 2, \"Inputs must be 2 dim matrices\" \n",
    "    assert A.shape[1] == B.shape[0], \"Incompabile matrices for GeMMs\"\n",
    "\n",
    "    A.contiguous()\n",
    "    B.contiguous()\n",
    "\n",
    "    # ----- Grid, Blocks, Warps and EUs --------------\n",
    "    x_grid = tl.cdiv(M, BLOCK_M)\n",
    "    y_grid = tl.cdiv(N, BLOCK_N)\n",
    "    z_grid = splitK\n",
    "    grid = (x_grid, y_grid, z_grid)\n",
    "\n",
    "    # --------  kernel lauch --------\n",
    "    splitK_kernel[grid](\n",
    "        A, \n",
    "        B, \n",
    "        out,\n",
    "        A.stride(0), \n",
    "        A.stride(1), \n",
    "        B.stride(0), \n",
    "        B.stride(1),\n",
    "        BLOCK_M, \n",
    "        BLOCK_N, \n",
    "        splitK\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def main():\n",
    "    M = 128\n",
    "    K = 4096\n",
    "    N = 8192\n",
    "    splitK = 32\n",
    "\n",
    "    # Activation Matrix = [M, K] = [128, 4096] a skinny gemm since K << M, N\n",
    "    # Weight Matrix = [K, N] = [4096, 8192]\n",
    "\n",
    "    A = torch.randn(size=(M, K), dtype=torch.float16).to('cuda')\n",
    "    W = torch.randn(size=(K, N), dtype=torch.float16).to('cuda')\n",
    "\n",
    "    C_ref = torch.matmul(A, W)\n",
    "    C_splitK = splitk_gemm(A, W, splitK)\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "relevant hyperparameters for our kernel such as tile sizes, number of warps and the number of pipeline stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Metrics \n",
    "Metrics SplitK Data Parallel\n",
    "Latency(us) 27.90us 52.93us\n",
    "Global Memory Throughput(GB/s) 313 GB/s 161 GB/s Higher is better\n",
    "Grid Size(Thread block launch count) 512 128 \n",
    "Registers 92 150\n",
    "Shared Memory Usage(KB) 102.40KB 167.94KB\n",
    "Block Limit (Registers) 5 3\n",
    "Block Limit (SMEM) 5 2\n",
    "Achieved Occupancy 27.75 7.55\n",
    "SM Utilization 43.05% 20.75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Internals \n",
    " \n",
    "```py\n",
    "offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "a_ptr = A + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
    "```\n",
    "\n",
    "and \n",
    "\n",
    "```py\n",
    "# The make_block_ptr function wil use the provided block size to internally compute the full list of off_ams. \n",
    "offs_am = pid_m * BLOCK_M  \n",
    "a_block_ptr = tl.make_block_ptr(\n",
    "    base=A,\n",
    "    shape=(M, K),\n",
    "    strides=(stride_am, stride_ak),\n",
    "    offsets=(offs_m, 0),\n",
    "    block_shape=(BLOCK_M, BLOCK_K),\n",
    "    order=(1, 0)\n",
    ")\n",
    "```\n",
    "\n",
    "are equivalent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "a_block_ptr = tl.make_block_ptr(\n",
    "    base=A,                                      # base pointer to the parent tensor\n",
    "    shapes=(M, K),                               # Shape of the parent tensor\n",
    "    strides=(stride_mm, stride_mk),              # strides for the parent tensor\n",
    "    offsets=(pid_m * BLOCK_M, pid_k * SPLIT_K),  # offsets to the block ie tile pointer start\n",
    "    block_shapes=(BLOCK_M, BLOCK_K),             # The shapes/size of the block \n",
    "    order=(0, 1)                                 # (N, T) row major\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
