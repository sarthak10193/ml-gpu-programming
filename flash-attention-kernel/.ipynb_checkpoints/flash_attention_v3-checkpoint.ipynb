{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention V3 Deepdive\n",
    "\n",
    "Dao et al. introduced FlashAttention, a novel tiling strategy for parallelizing attention that eliminates intermediate\n",
    "reads/writes to slow global memory through fusing all of the attention operations into a single GPU kernel. Dao\n",
    "[15] restructured the algorithm as FlashAttention-2 to also parallelize over the sequence length dimension and\n",
    "perform the inner loop of the forward pass over blocks of the key and value matrices, thus improving the occupancy\n",
    "and distribution of work on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q [k, :]  $  --> The Kth row of  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Autograd API\n",
    "**FWD PASS SETUP & LAUNCH**\n",
    "1. Asserts: Check if the shapes of Q, K, V tensors are good. Check if values are the type and shape they should be\n",
    "2. Allocate Input and output buffers.\n",
    "3. Decide how to parallelize the workload: grids, warps, compute units, numblocks, block size w.r.t the workload under consideration\n",
    "4. Launch FWD Triton Kernel using the `grid[]()`\n",
    "5. Save CTX needed for BWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import torch \n",
    "\n",
    "def is_hip():\n",
    "    return True if triton.runtime.driver.active.get_current_target().backend == 'hip' else False\n",
    "\n",
    "\n",
    "class _attention(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, v, causal:bool, softmax_scale: float):\n",
    "        \"\"\"\n",
    "        the ctx object is used to build ctx that is required during the bwd pass\n",
    "        we'll use ctx.save_for_backward(...) to do this. \n",
    "        \"\"\"\n",
    "        # 1. asserts\n",
    "        # 2. Allocate Input and output buffers \n",
    "        # 3. grids, warps, compute units, numblocks, block size\n",
    "        # 4. Launch\n",
    "\n",
    "\n",
    "        ### ------ Check Inputs --------------\n",
    "        HEAD_EMB_DIM_Q == HEAD_EMB_DIM_K == q.shape[-1] == k.shape[-1]  # q: [B, S, H, E]\n",
    "        HEAD_EMB_DIM_V == v.shape[-1]\n",
    "\n",
    "        assert HEAD_EMB_DIM_Q == HEAD_EMB_DIM_K == HEAD_EMB_DIM_V\n",
    "        assert HEAD_EMB_DIM_K in {16, 32, 65, 128, 256}, \"Only head size of 16, 32, 65, 128, 256 are supported\"  # why are only these supported ??\n",
    "\n",
    "        ### ------- Allocate Buffers ----------\n",
    "        o = torch.empty_like(q)      # whis the output the shape of q tensor ?\n",
    "        stage = 3 if causal else 1   # causal requires special considerations such as masking\n",
    "        \n",
    "        ### ------ Grid, Warps, EUs, Blocks etc ---------\n",
    "        # SM == EU; WARPS (32 threads) == WAVEFRONTS (64 threads) ; OCCUPANCY == WAVES_PER_EU == ACTIVE WARPS PER SM\n",
    "        # How does this affect perf\n",
    "            # - register pressure \n",
    "        if is_hip(): # \n",
    "            waves_per_eu = 3 if HEAD_EMB_DIM_K <= 64 else 2 # why is this set this way ?\n",
    "            extra_kern_args = {\"waves_per_eu\": waves_per_eu, \"allow_flush_denorm\": True}\n",
    "\n",
    "        # The grid can be 3D. \n",
    "        # Since Q.shape = [BATCH, NUM_HEADS, SEQ_LEN, EMB_DIM] \n",
    "        # we know we can parallelize across the SEQ_LEN dim. since BATCH * NUM_HEADS essentially is our BATCH DIM\n",
    "        x_grid = triton.cdiv(q.shape[2], BLOCK_M)  # BLOCK_M is the tile size. If seq len = 1024 and BLOCK_M=128 then NUM_BLOCKS=8. Therefore each block will process BLOCK_M elements of the seq.\n",
    "        y_grid = q.shape[0] * q.shape[1]           # BATCH * NUM_HEADS, since these are 100 % indepenedent.\n",
    "        z_grid = 1 \n",
    "        grid = (x_grid, y_grid, z_grid)\n",
    "        M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n",
    "\n",
    "        _attn_fwd[grid]( # what is this call ?\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            softmax_scale,\n",
    "            M,\n",
    "            o,  #\n",
    "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access. \n",
    "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access.\n",
    "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access.\n",
    "            o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access.\n",
    "            q.shape[0], q.shape[1],  #\n",
    "            N_CTX=q.shape[2],  #\n",
    "            HEAD_DIM=HEAD_DIM_K,  #\n",
    "            STAGE=stage,  #\n",
    "            **extra_kern_args\n",
    "        )\n",
    "\n",
    "        ### Save context for BWD\n",
    "        ctx.save_for_backward(q, k, v, o, M)\n",
    "        ctx.grid = grid\n",
    "        ctx.sm_scale = sm_scale\n",
    "        ctx.HEAD_DIM = HEAD_DIM_K\n",
    "        ctx.causal = causal\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward():\n",
    "        \"\"\"\n",
    "        You can make mistakes while implemetning the bwd pass. To help with this we can use the .gradcheck() to use \n",
    "        infinite small internal [f(a+h) - f(a-h)] / 2h \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "attention = _attention.apply\n",
    "\n",
    "# python api\n",
    "def flash_attention_v2(batch_size: int, num_heads: int, seq_len: int, emb_dim: int, causal:bool, mode):\n",
    "    dtype = torch.float16\n",
    "    device='cuda'\n",
    "\n",
    "    q = torch.randn((batch_size, num_heads, seq_len, emb_dim), dtype=dtype, device=device) \n",
    "    k = torch.randn((batch_size, num_heads, seq_len, emb_dim), dtype=dtype, device=device) \n",
    "    v = torch.randn((batch_size, num_heads, seq_len, emb_dim), dtype=dtype, device=device) \n",
    "    \n",
    "    if mode == 'fwd':\n",
    "        q = q.to(torch.float8_e5m2) # why e5m2 for FWD pass ?\n",
    "        k = k.to(torch.float8_e5m2)  # what is _nuz ?\n",
    "\n",
    "    softmax_scale = 1.3  \n",
    "    fn = lambda: attention(q, k, v, causal, softmax_scale) # why use a lambda function here ?\n",
    "    ms = triton.testing.do_bench(fn) # benchmark the op and report time in milli seconds. \n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_HEADS = 16\n",
    "    SEQ_LEN = 4096\n",
    "    EMB_DIM = 1024\n",
    "\n",
    "    # We have many variations to support\n",
    "    # 1. QKV - Stanards MHA. Most efficient since the tensors are packed and a giant matmul can be launched.\n",
    "    # 2. QK and V: useful for GQA\n",
    "    # 3. Q and K and V\n",
    "    flash_attention_v2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attention_fwd(\n",
    "    q, k, v, \n",
    "    softmax_scale, \n",
    "    output, \n",
    "    stride_qz, stride_qh, stride_qm, stride_qk,  #\n",
    "    stride_kz, stride_kh, stride_kn, stride_kk,  #\n",
    "    stride_vz, stride_vh, stride_vk, stride_vn,  #\n",
    "    stride_oz, stride_oh, stride_om, stride_on,  #\n",
    "    Z, H, N_CTX,  #\n",
    "    HEAD_DIM: tl.constexpr,  #\n",
    "    BLOCK_M: tl.constexpr,   #\n",
    "    BLOCK_N: tl.constexpr,   #\n",
    "    STAGE: tl.constexpr      #\n",
    "):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BWD PASS SETUP & LAUNCH**\n",
    "1. Asserts: Check if the shapes of Q, K, V tensors are good. Check if values are the type and shape they should be\n",
    "2. Allocate Input and output buffers.\n",
    "3. Decide how to parallelize the workload: grids, warps, compute units, numblocks, block size w.r.t the workload under consideration\n",
    "4. Launch FWD Triton Kernel using the `grid[]()`\n",
    "5. Save CTX needed for BWD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
