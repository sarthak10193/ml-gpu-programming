{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention Deepdive\n",
    "\n",
    "## Overview\n",
    "Flash attention has evolved over the last 3 years with v1, v2 and v3. A table with the key differences is provided at the end of this notebook. \n",
    "- Analysis and Limitations of Vanilla attention\n",
    "- Softmax and Online Softamx\n",
    "- More on \n",
    "- Vanilla attention deepdive\n",
    "- Flash Attention\n",
    "\n",
    "Discuss Advance topics\n",
    "- flex attention \n",
    "- deepseek attention methods\n",
    "- Ohter customization from character ai etc. \n",
    "\n",
    "### Why Flash Attention?\n",
    "* Vanilla attention is $O(N^2)$ in runtime and memory. As the need for larger and larger seq lenghts grows, this quadratic relationship is a big bottleneck\n",
    "* Prior attempts have mostly targeted at ideas like FLOPS reduction (sparse attention) using modified versions of attention without taking into account SW-HW co-design.\n",
    "    * HBM is an order of magnitude slower than SMEM. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How\n",
    "Flash Attention V1 is inspired by the `DataMovement is all you need` paper. This paper analyzes the data movement and storage patterns of attention ops. The key takeaway is that vanilla attention is memory-bound (figure 1). \n",
    "\n",
    "* **IO Aware**: Takes into account the GPU memory hierarchy and compute architecture. Basically move beyond the mathematical / PyTorch formulation of attention and think critically about the target devices (Mi300x, H100, H200 etc) memory and compute properties. \n",
    "    * **Tiling**: To take full advantage of the various memory capacities and bandwidth (HBM, SMEM, L2 Cache, Registers, etc), the attention algorithm is re-organized in a way that doesn't change the end result (ie exact). It uses \"math tricks\" to make the algo tiling compatible. **NOTE:** By default the attention function is not \"tilable\" due to the non-associate and non-distributive nature of the softmax function. More on this later. \n",
    "\n",
    "* **Activation Recomputation**\n",
    "How do you get here ? Using the data movement paper + facts/research observations on Elementiwise vs matmul parts in terms of memory bound. \n",
    "\n",
    "\n",
    "In simpler terms, the flash attention algo provides a **fused kernel implememtation** of the original attention function $$O = dropout(mask(softmax(Q \\dot K^T))V))$$. \n",
    "\n",
    "Formally, Dao et al. introduced FlashAttention, a novel tiling strategy for parallelizing attention that eliminates intermediate\n",
    "reads/writes to slow global memory through fusing all of the attention operations into a single GPU kernel. Dao\n",
    "[15] restructured the algorithm as FlashAttention-2 to also parallelize over the sequence length dimension and\n",
    "perform the inner loop of the forward pass over blocks of the key and value matrices, thus improving the occupancy\n",
    "and distribution of work on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome \n",
    "As a result of Flash attention 1\n",
    "* Wall clock time speedup of 2- 4x over optimized baselines \n",
    "* O(N) in memory \n",
    "* About 25-50% FLOPS utilization (compared to theoretical max) over a mere 10% FLOPS utilization with vanilla attention. Note that this flops util is still low if you compare it to GeMMs. We'll address some of this in Flash Attention v2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisit GPU Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Memory Hierarchy\n",
    "We'll take into account the realization that HBM r/w (2TB / sec) are an order of magnitude slower than SMEM r/w (20TB / sec)\n",
    "\n",
    "| Memory Level         | H100                   | MI300X                 | Description                                                                                  |\n",
    "|-----------------------|-------------------------|-------------------------|----------------------------------------------------------------------------------------------|\n",
    "| Registers            | 256KB per SM, ~90 TB/s  | 256KB per CU, ~100 TB/s| Fastest memory, private to each thread. Stores variables and intermediate values for execution.|\n",
    "| Shared Memory (SMEM) | upto 258KB per SM, ~50 TB/s  | 256KB per CU, ~60 TB/s | Programmable memory local to thread blocks. Used for data sharing and reuse within a block. Nvidia combines the SMEM and L1 cache into a unified memory region that the programmer can control allocation ratio for. For applications with high thread-level data sharing (e.g., matrix multiplication), the GPU can allocate more of this 256 KB as shared memory. |\n",
    "| L1 Data Cache             | 128KB per SM, ~50 TB/s  | 128KB per CU, ~60 TB/s | Automatically caches frequently accessed local and global memory. Shared within an SM/CU.    |\n",
    "| L2 Cache             | 50MB, 37.5 TB/s         | 256MB for 8 XCDs, 50 TB/s          | Last-level cache before accessing HBM. Shared across all SMs/CUs on the GPU. Also called as AMD Infinity Cache.               |\n",
    "| HBM3                 | 80GB, 3.35 TB/s         | 192GB, 5.3 TB/s        | High Bandwidth Memory used for storing large datasets and models. Accessible by all SMs/CUs.|\n",
    "\n",
    "**MI300x**\n",
    "* WARPS PER SM = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Attention \n",
    "### Formulation\n",
    "Remember that the IO unaware formulation of attention is as follows:\n",
    "\n",
    "$\\text{Let } Q, K, V \\in \\mathbb{R}^{N \\times d} \\text{ be the query, key and value input sequences}$\n",
    "$ \\text{associated to a single head, where } N \\text{ is the sequence length and } d \\text{ is the} $\n",
    "$ \\text{head dimension. Then the attention output } O \\text{ is computed as:} $ \n",
    "\n",
    "\\begin{align*}\n",
    "& S = \\alpha QK^\\top \\in \\mathbb{R}^{N \\times N}, && \\\\ \n",
    "& P = \\text{softmax}(S) \\in \\mathbb{R}^{N \\times N}, && \\\\ \n",
    "& O = PV \\in \\mathbb{R}^{N \\times d}, &&\n",
    "\\end{align*}\n",
    "\n",
    "Then give B Batches and H heads (MHA) for each of Q, K, V we get the following shapes: \n",
    "* Q: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "* K: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "* V: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "\n",
    "You can parallelize the GeMM across BATCH and HEAD dimensions. \n",
    "\n",
    "\n",
    "### Limitation\n",
    "\n",
    "#### I - Attention is $O (N^2)$ in Memory Complexity \n",
    "**Materiallization**: if N is too large (for say a million context length) then it would **NOT be possible** at all to fit the entire $Q \\dot K^T$ in the `SMEM`. Since $Q \\dot K^T \\in N, N$ this means, that Attention is $O(N^2)$ in memory complexity, where N is the number of tokens/sequence len.\n",
    "\n",
    "* N = 8192\n",
    "* B = 1\n",
    "* d = 128 [does not matter in this calclulation]\n",
    "* NUM HEADS = 16\n",
    "* Precision = bfp16 (2 bytes)\n",
    "* Then NxN matrix = 8192 X 8192 X 1 X 16 elements = 1073741824 where each element takes 2 bytes.\n",
    "* <mark>Total memory required if NxN is materialized ~2148 MB. </mark>. This is TOO large to fit in SMEM which is usually in KBs.\n",
    "\n",
    "<u>Understanding the Space & Time Complexity of Attention<u>\n",
    "* Lower Bounds on IO access\n",
    "* Parameterization\n",
    "\n",
    "#### II -  Attetion is IO Bound  \n",
    "**IO Bound**:  Standard computation would require 6 trips (3 loads and 3 stores) from the HBM and is memory bound. \n",
    "* Load $Q$ & $K$   --> compute $X = Q \\dot K^T$   --> Store $X$     \n",
    "* Load $X$         --> compute $A = softmax(X)$   --> Store $A$\n",
    "* Load $A$, $V$    --> compute $O = X  V$          --> Store $O$\n",
    "\n",
    "We already know that HBM access is 10x slower than SMEM access. \n",
    "\n",
    "Add the distribution of time vs flops chart here—link to glossary.\n",
    "\n",
    "### Addressing the Limiations\n",
    "1. Tiling\n",
    "Matrix Mul Op have the nice property of **Associativity** & **Distributivity**. Therefore the $Q \\dot K^T$ can be tiled achieving massive paralllism on GPUs. \n",
    "2. However, the subsequent operation `SOFTMAX` CANNOT be tiled. Additionally, using the 'Safe Softmax' operations requires us to scan the entire sequence length for the max value. We'll use online softmax to make softmax tilable and use activation recomputation. Doing so we'll be able to compute the entire Attention OP without materialization the intermediate S and P matrices on the HBM.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online Softmax\n",
    "**Safe Softmax** Additionally in the real world we use  `Safe Softmax` operation. Safe Softmax **subtracts the MAX values to avoid overflow  and underflow** given a certain bit-width. Ex: IN fp16 (2bytes) the max value can only be 65556. Therefore if $x_{i}=12$ then  $e^{12}$ is too large to fit into the dynamic range of FP16. \n",
    "\n",
    "* fp16 is with: `1 signed bit |  5 exponent bits |  10 mantissa` therefore very high precision but lower dynamic range \n",
    "* bf16 is with: `1 signed bit |  8 exponent bits | 7 mantissa bit`  therefore very low precision but higher dynamic range\n",
    "\n",
    "$$\\text{Safe Softmax}(x_i) = \\frac{e^{x_i - M}}{\\sum_{j=1}^{n} e^{x_j - M}} \\quad \\text{where } M = \\text{max} (x_i)$$\n",
    "\n",
    "\n",
    "The online softmax (by Milakov et al) title `Online normalizer calculation for softmax` was introduced by Nvidia in 2018. \n",
    "\n",
    "1. Take care of overflows / underflow by subtracing the max value. IF the bit width is 16 (bf16, fp16).\n",
    "2. \n",
    "\n",
    "The **implication / advantage of doing Online Softmax** is that equation (7) and equation (8) can be fused since you don't require $m_{N}$ any more and only require $m_{i}$ and $m_{i-1}$. All of this is hinged on the fact that $d_{N} == d^{`}_{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 =                # \n",
    "s2 = [5, 1, 6]\n",
    "row_max_s1 = max(s1)\n",
    "row_max_s2 = max(s2)\n",
    "m = max(row_max_s1, row_max_s2) # GLOBAL MAXIMIUM\n",
    "l = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the space-time complexity of Attention \n",
    "* Lower Bounds on IO access\n",
    "* Parameterization\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Autograd API\n",
    "This section creates the python wrapper that integrates into Pytorch by inheriting the `torch.autograd.Function` class and implementing the `foward()` and `backward()` member functions. \n",
    "\n",
    "**FWD PASS SETUP & LAUNCH**\n",
    "\n",
    "The broad recipe we follow: \n",
    "1. Asserts: Check if the shapes of Q, K, V tensors are good. Check if values are the type and shape they should be.\n",
    "2. Allocate Input and Output buffers.\n",
    "3. Decide how to parallelize the workload - reason about grids, warps, compute units, numblocks, block size w.r.t the workload under consideration\n",
    "4. Launch FWD Triton Kernel using the `kernel_name[grid](<kernel params>)`\n",
    "5. Save CTX needed for BWD. This context would not be necessary if were doing inference. The context is essentially data that the BWD pass will require. \n",
    "- LSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Let } Q, K, V \\in \\mathbb{R}^{N \\times d} \\text{ be the query, key and value input sequences}$\n",
    "$ \\text{associated to a single head, where } N \\text{ is the sequence length and } d \\text{ is the} $\n",
    "$ \\text{head dimension. Then the attention output } O \\text{ is computed as:} $ \n",
    "\n",
    "\\begin{align*}\n",
    "& S = \\alpha QK^\\top \\in \\mathbb{R}^{N \\times N}, && \\\\ \n",
    "& P = \\text{softmax}(S) \\in \\mathbb{R}^{N \\times N}, && \\\\ \n",
    "& O = PV \\in \\mathbb{R}^{N \\times d}, &&\n",
    "\\end{align*}\n",
    "\n",
    "Then give B Batches and H heads (MHA) for each of Q, K, V we get the following shapes: \n",
    "* Q: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "* K: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "* V: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "\n",
    "1. Batches and the heads are to be treated independently [heads don't pass around info]. Therefore, the effective matrix becomes `[B*H, N, d]`where `B*H` acts as the batch dimension. In addition, **Flash Attention 2** parallelizes along the sequence dimension as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import torch \n",
    "\n",
    "def is_hip():\n",
    "    return True if triton.runtime.driver.active.get_current_target().backend == 'hip' else False\n",
    "\n",
    "\n",
    "class FusedAttention(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, v, causal:bool, softmax_scale: float):\n",
    "        \"\"\"\n",
    "        the ctx object is used to build ctx that is required during the bwd pass\n",
    "        we'll use ctx.save_for_backward(...) to do this. \n",
    "        \"\"\"\n",
    "        ### ------ 1. Check Inputs --------------\n",
    "        HEAD_EMB_DIM_Q = HEAD_EMB_DIM_K = q.shape[-1] = k.shape[-1]  # q: [B, S, H, E]\n",
    "        HEAD_EMB_DIM_V = v.shape[-1]\n",
    "\n",
    "        assert HEAD_EMB_DIM_Q == HEAD_EMB_DIM_K == HEAD_EMB_DIM_V, \"All heads must have the same embedding dimension\"\n",
    "        assert HEAD_EMB_DIM_K in {16, 32, 64, 128, 256}, \"Only head size of 16, 32, 65, 128, 256 are supported\"  \n",
    "        # why are only these supported ??\n",
    "        # - usually we prefer powers of 2 because of GPU SMs (ALUs)being a power of 2 for distirbuting computation. \n",
    "        # -  \n",
    "\n",
    "        ### ------- 2. Allocate Buffers ----------\n",
    "        o = torch.empty_like(q)      # whis the output the shape of q tensor ?\n",
    "        stage = 3 if causal else 1   # causal requires special considerations such as masking. If masking is required then additional memory needs to be allocated for the boolean mask. \n",
    "        \n",
    "        ### ------ 3. Grid, Warps, EUs, Blocks etc ---------\n",
    "        # SM == EU; \n",
    "        # WARPS (32 threads) == WAVEFRONTS (64 threads) ; \n",
    "        # OCCUPANCY == WAVES_PER_EU == ACTIVE WARPS PER SM\n",
    "        # How does this affect perf\n",
    "            # - register pressure \n",
    "        if is_hip(): \n",
    "            # Increase the active warps count if ...\n",
    "            # Decrease the warp count if ...\n",
    "            waves_per_eu = 3 if HEAD_EMB_DIM_K <= 64 else 2 \n",
    "            extra_kern_args = {\"waves_per_eu\": waves_per_eu, \"allow_flush_denorm\": True}\n",
    "\n",
    "        # The grid can be 3D. \n",
    "        # Since Q.shape = [BATCH(0, B), NUM_HEADS(1, H), SEQ_LEN(2, L), EMB_DIM(3, d)] \n",
    "        # we know we can parallelize across the SEQ_LEN dim. since BATCH * NUM_HEADS is our BATCH DIM\n",
    "        x_grid = triton.cdiv(q.shape[2], BLOCK_M)  # BLOCK_M is the tile size. If seq len = 1024 and BLOCK_M=128 then NUM_BLOCKS=8. Therefore each block will process BLOCK_M elements of the seq.\n",
    "        y_grid = q.shape[0] * q.shape[1]           # BATCH * NUM_HEADS, since these are 100 % indepenedent.\n",
    "        z_grid = 1 \n",
    "        grid = (x_grid, y_grid, z_grid) # parallelization was done from inwards to outwards. \n",
    "        M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n",
    "\n",
    "        ### ------ 4. Launch ---------\n",
    "        _attn_fwd[grid](\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            softmax_scale,  # softmax scale. \n",
    "            M,  # causal mask tensor\n",
    "            o,  #   \n",
    "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access. \n",
    "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access.\n",
    "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access.\n",
    "            o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access.\n",
    "            q.shape[0], q.shape[1],  #\n",
    "            N_CTX=q.shape[2],  #\n",
    "            HEAD_DIM=HEAD_DIM_K,  #\n",
    "            STAGE=stage,  \n",
    "            **extra_kern_args\n",
    "        )\n",
    "\n",
    "        ### Save context for BWD\n",
    "        ctx.save_for_backward(q, k, v, o, M)\n",
    "        ctx.grid = grid\n",
    "        ctx.sm_scale = sm_scale       # normalization factro diag(l)^-1\n",
    "        ctx.HEAD_DIM = HEAD_DIM_K\n",
    "        ctx.causal = causal\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward():\n",
    "        \"\"\"\n",
    "        You can make mistakes while implemetning the bwd pass. To help with this we can use the .gradcheck() to use \n",
    "        infinite small internal [f(a+h) - f(a-h)] / 2h. In addition to that people use tools like PySolver.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# python api\n",
    "def flash_attention_v2(\n",
    "    batch_size: int,\n",
    "    num_heads_q: int,\n",
    "    num_heads_kv: int, \n",
    "    seq_len: int,\n",
    "    emb_dim: int,\n",
    "    causal:bool,\n",
    "    mode: str,\n",
    "    dropout: bool\n",
    "):\n",
    "    dtype = torch.float16\n",
    "    device='cuda'\n",
    "\n",
    "    q = torch.randn((batch_size, num_heads_q, seq_len, emb_dim), dtype=dtype, device=device) \n",
    "    k = torch.randn((batch_size, num_heads_kv, seq_len, emb_dim), dtype=dtype, device=device) \n",
    "    v = torch.randn((batch_size, num_heads_kv, seq_len, emb_dim), dtype=dtype, device=device) \n",
    "    \n",
    "    if mode == 'fwd':\n",
    "        q = q.to(torch.float8_e5m2) # why e5m2 for FWD pass ?\n",
    "        k = k.to(torch.float8_e5m2)  # what is _nuz ?\n",
    "\n",
    "    softmax_scale = 1.3  \n",
    "    fn = lambda: FusedAttention.apply(q, k, v, causal, softmax_scale) # why use a lambda function here ?\n",
    "    ms = triton.testing.do_bench(fn) # benchmark the op and report time in milli seconds. \n",
    "\n",
    "    \n",
    "def main():\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_HEADS_Q = 16\n",
    "    NUM_HEAD_KV = 16\n",
    "    SEQ_LEN = 4096\n",
    "    EMB_DIM = 1024\n",
    "\n",
    "    # Add support for QKV, QK and V, Q and K and V\n",
    "    flash_attention_v2(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_heads_q=NUM_HEADS_Q, \n",
    "        num_heads_kv=NUM_HEAD_KV,\n",
    "        seq_len=SEQ_LEN,\n",
    "        emb_dim=EMB_DIM,\n",
    "        causal=False,\n",
    "        mode='fwd',\n",
    "        dropout=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton Kernel\n",
    "\n",
    "### Tile Programming\n",
    "As shown above using the **online softmax** provides us with **eventual consistency**. Therefore we can now parallelize across \n",
    "\n",
    "* X: seq length\n",
    "* Y: batch*head\n",
    "* Z: 1 [no parallelism]\n",
    "\n",
    "<mark>Each **CUDA THREAD BLOCK** gets a **UNIQUE PROGRAM ID**<mark>\n",
    "\n",
    "<img src=\"diagrams/fa_triton_program_id_mapping.png\" alt=\"Description\" width=\"700\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Flash attention v1** paralellizes over the batch size and the number of heads since this each head is indepdendent. 1 threadblock is used to process 1 attention head.  Therefore given a `batch size=B` and `number of heads=H`, we have **total number of threadblocks being launched = H * B**\n",
    "* `A100 has 108SM SM`,  `H100 has 114SMs` and `Mi300x has 304SMs`. Therefore, the above scheduling of a) parallelization across heads b) 1 head per SM is effciiency when Total heads >= 100 since we end up using most compute resources of the GPU.\n",
    "* However, in the case of long sequences (Llama2 8192) we usually have to go with both smaller batch sizes and less number of heads. Therefore this scehduling becomes inefficient. We'll fix this in **FlashAttention 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "BLOCK_M: Seq Tile size\n",
    "BLOCK_N\n",
    "\n",
    "\n",
    "Block level partial softmax is computed. For the partial softmax to converge to actual softmax we need to maintain 2 statistics - `m(x)`. We do this recursively over all the N/BLOCK_SIZE number of blocks,  accumulatating the results in the accumulator O.\n",
    "\n",
    "**Notatation**\n",
    "1. Given a shared memory size of $M$ we determined the block size to be  \n",
    "\n",
    "    BLOCK_SIZE = ($B_c$,   $B_r$) where $B_c = \\left\\lceil\\frac{M}{4d}\\right\\rceil$ and $B_r = \\min\\left(\\left\\lceil\\frac{M}{4d}\\right\\rceil, d\\right) $\n",
    "\n",
    "   We'll tune the block_size and find the best M based on our GPU target arch.\n",
    "\n",
    "3. Init Tensors\n",
    "    * **Output tensor**: Allocate 0s for $O$ where $O = PV \\in \\mathbb{R}^{N \\times d}$\n",
    "    * m, l\n",
    "4. Next let's chunk the Q, K, and V matrices into block\n",
    "    * **row wise split**: divide the Q matrix into chunks ($Q_1, Q_2, .... Q_{Tr}$) where each chunk $Q_i$ is $B_r$ X $d$ size. We have a total of $T_r$ such chunks where $T_r = N / B_r$\n",
    "    * **col wise split**: divide the K and V matrix into chunks ($K_1, K_2, .... K_{Tc}$) where each chunk $K_i$ is $B_c$ X $d$ size. We have a total of $T_c$ such chunks where $T_c = N / B_c$\n",
    "\n",
    "5. Block wise output $O$ \n",
    "\n",
    "6. Implement **Fused Kernel** using Tiling and Online Softmax \n",
    "    * **Outer Loop**: Load $K_j$ and $V_j$ for each j in [1 .... $Tc$]\n",
    "    * **Inner Loop**: Load $Q_i$ for each i in [1 ... $Tr$]\n",
    "     \n",
    "\n",
    "<img src=\"diagrams/flash_attention_algo_v1.png\" alt=\"Description\" width=\"900\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\" style=\"vertical-align: top;  background-color: #f6f8fa; \">\n",
    "\n",
    "![Image Title](diagrams/inner_loop_attn.png)\n",
    "\n",
    "</td>\n",
    "<td width=\"55%\" style=\"vertical-align: top; background-color: #f6f8fa; padding: 10px; border-radius: 5px; font-size: 0.85em\">\n",
    "\n",
    "```python\n",
    "# Pseudocode\n",
    "for b_c in range(0, N, B_c):  # Outer loop over K, V blocks. Each block is of size (B_c, d). Total blocks is ceil_div(N/B_c)\n",
    "    # Load K, V blocks each of size (B_c x d) into SRAM \n",
    "    K_block = K[b_c:b_c+B_c, :]  # ie B_c tokens of the seq across all d dimension. Remember, d is usually small and seq can be very large. \n",
    "    V_block = V[b_c:b_c+B_c, :]  # B_c × d block\n",
    "    \n",
    "    for b_r in range(0, N, B_r):     # Inner loop over Q blocks\n",
    "        # Load Q block into SRAM. Each Q block is of shape (B_r, d)\n",
    "        Q_block = Q[b_r:b_r+B_r, :]  # B_r × d block\n",
    "        \n",
    "        # Compute attention for this block\n",
    "        S = Q_block @ K_block.T      # (B_r, d) @ (d, B_c) = (B_r, B_c) attention scores\n",
    "        P = softmax(S)               # (B_r × B_c) attention weights\n",
    "        O_block = P @ V_block        # (B_r, B_c) @ (B_c × d) = (B_r, d) output block.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Left: The GPU is a hierarchical computing and memory system. \n",
    "    * The HBM BW (gbps / seconds) is an order of magnitude (10x) slower that the the SMEM\n",
    "    * However, the the SMEM memory is 100x smaller than the HBM in storage (HBM: 80GB H100, 192 GB MI300x vs SMEM: )\n",
    "2. Middle: This is the core logic / kernel `_attention_fwd` of the flash attention algorithms in terms. This helps avoid the materialization of the N x N matrix \n",
    "    * The **outer for loop** loads the \n",
    "    \n",
    "<img src=\"diagrams/inner_loop_attn_paper.png\" alt=\"Description\" width=\"1000\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FWD KERNEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (4130513075.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 39\u001b[0;36m\u001b[0m\n\u001b[0;31m    offsets = (start_m * BLOCK_M, 0)   # offsets to the block ==>  how to access the next block\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def _attn_fwd_inner(\n",
    "    acc, \n",
    "    li, \n",
    "    mi, \n",
    "    q,  # [BLOCK_M, HEAD_DIM]\n",
    "    K_block_ptr, V_block_ptr, \n",
    "    start_m, \n",
    "    qk_scale,  # softmax scale, default to sqrt(HEAD_DIM)\n",
    "    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  #\n",
    "    STAGE: tl.constexpr, \n",
    "    offs_m: tl.constexpr, offs_n: tl.constexpr,  #\n",
    "    N_CTX: tl.constexpr,  # The operation is parallelized by dividing the sequence length (N_CTX) into blocks of size BLOCK_N.\n",
    "    fp8_v: tl.constexpr\n",
    "        \n",
    "): \n",
    "    \"\"\"\n",
    "    Load blocks of Q, K and V from HBM.\n",
    "    \"\"\"\n",
    "    if STAGE == 1:\n",
    "        lo, hi = 0, start_m * BLOCK_M\n",
    "    elif STAGE == 2:\n",
    "        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n",
    "        lo = tl.multiple_of(lo, BLOCK_M)\n",
    "    # causal = False, so keep the entire range of 0, N_CTX[no masking]\n",
    "    else:\n",
    "        lo, hi = 0, N_CTX\n",
    "    \n",
    "    # low, high marks the low/high index position over seq len. \n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _attention_fwd(\n",
    "    Q,                  # Q:  M, K matrix\n",
    "    K,                  # Kt: K, N matrix\n",
    "    V,                  # V:  K, N matrix\n",
    "    softmax_scale, \n",
    "    OUT,                # O: M, N matrix\n",
    "    stride_qz, stride_qh, stride_qm, stride_qk,  # jump by stride_qz to get to next batch and stride_qh to get the next head. \n",
    "    stride_kz, stride_kh, stride_kn, stride_kk,  # jump by stride_kk to get to next col in the Key matrix. \n",
    "    stride_vz, stride_vh, stride_vk, stride_vn,  # \n",
    "    stride_oz, stride_oh, stride_om, stride_on,  #\n",
    "    B,                  # Batch Size \n",
    "    H,                  # Number of Heads\n",
    "    N_CTX,      \n",
    "    lse,         # \n",
    "    dropout_p, \n",
    "    is_causal, \n",
    "    HEAD_DIM: tl.constexpr,  # d\n",
    "    BLOCK_M: tl.constexpr,   # Tile size in the QUERY SEQ DIMENSION\n",
    "    BLOCK_N: tl.constexpr,   # Tile size for Key/Value dimension\n",
    "    STAGE: tl.constexpr      #\n",
    "\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Currently there is no support for\n",
    "    1. Dropout\n",
    "        - Requires a random number generator per thread. Philox Seed \n",
    "    2. SWA, GQA\n",
    "    3. Variable sequence Lengths Layout (thd) --> (token, head,  dim_per_head)\n",
    "    4. Context Parallelism\n",
    "        - Due to intern token communication during attention, we need to all gather during fwd and reduce scatter during bwd\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Here we try to create a highly parallelizable \"instance of the program\". Therefore we must\n",
    "    1. Define the program with the correct way for a program instance to know what data to work [which block, which tile, which batch which head & so on]\n",
    "    2. \n",
    "    \"\"\"\n",
    "    # assume [BATCH(B/Z)=4, NUM_HEADS(H)=16, SEQ_LEN(L)=512, EMBED_DIM(d)=96]\n",
    "    start_m = tl.program_id(0)    # get the current program instance along x axis. Achieve parallelism by tiling  along seq dim [X-axis]\n",
    "    offset_bh = tl.program_id(1)  # get the current program instance along y axis.  Parallelism across the Batch * Head\n",
    "\n",
    "    off_z = off_hz // H  # batch offset. if off_hz = 11 and H = 8 then batch_id = 11 // 8 = 1 (2nd batch) \n",
    "    off_h = off_hz % H   # head offset.  if off_hz = 11 and H = 8 then head_id = 11 % 8 = 3 (4th head) \n",
    "\n",
    "    # with very large tensor sizes, the offsets can be huge. Use int64 to avoid overflow. \n",
    "    # Get Q given a head and batch iie qkv_offset = batch_id * stride_batch_Q + head_id * stride_head_Q. \n",
    "    # Programs gets dispatched onto the BLOCKs(The abstraction for Triton)\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base = Q + qkv_offset,             # offset Q by qkv_offset to get to the current batch * head for the current threadblock. \n",
    "        shape = (N_CTX, HEAD_DIM),         # Shape of the parent tensor (seqlen tile size , head_dim), helps with bounds checking when reading\n",
    "        strides = (stride_qm, stride_qk),  # The strides of the parent tensor [SeqLen X EmbedDim] ==> how to jump rows and cols of Q [mk] matrix. Helps with info contigous memory storage & access. \n",
    "        offsets = (start_m * BLOCK_M, 0)   # offsets to the block ==>  how to access the next block\n",
    "        block_shape = (BLOCK_M, HEAD_DIM)  # Size of the block to load from HBM to SMEM / LDS [tile size of seqlen, d]\n",
    "        order = (1, 0)                     # The order of the original data format [TN format]\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base = K + qkv_offset,             # The base pointer to the parent tensor\n",
    "        shape = (N_CTX, HEAD_DIM),         # Shape of the parent tensor (seqlen tile size , head_dim)\n",
    "        strides = (stride_kk, stride_kn),  # The strides of the parent tensor [EmbedDim X SeqLen] ==> How to jump rows and cols of K [kn] matrix \n",
    "        offsets = (0, 0)                   # offsets to the block # why are offserts 0, 0 ?\n",
    "        block_shape = (HEAD_DIM, N_CTX)    # HEAD DIM, SEQ TILE SIZE\n",
    "        order = (0, 1)                     # The order of the original data format\n",
    "    )\n",
    "\n",
    "    v_order: tl.constexpr = (0, 1) if V.dtype.element_ty == tl.float8e5 else (1, 0)\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base = V + qkv_offset,             # The base pointer to the parent tensor\n",
    "        shape = (N_CTX, HEAD_DIM),         # Shape of the parent tensor (seqlen tile size , head_dim)\n",
    "        strides = (stride_vk, stride_vn),  # The strides of the parent tensor [EmbedDim X SeqLen] ==> How to jump rows and cols of V [kn] matrix \n",
    "        offsets=(0, 0),                    # offsets to the block\n",
    "        block_shape = (BLOCK_N, HEAD_DIM)  # SEQ TILE SIZE , HEAD DIM\n",
    "        order = v_order                    # The order of the original data format\n",
    "    )\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=OUT + qkv_offset,\n",
    "        shape=(N_CTX, HEAD_DIM),           # N X d\n",
    "        strides=(stride_om, stride_on),    # The strides of the parent tensor [EmbedDim X SeqLen] ==> How to jump rows and cols of O [mn] matrix \n",
    "        offsets=(start_m * BLOCK_M, 0),    # Output BLOCKS\n",
    "        block_shape=(BLOCK_M, HEAD_DIM),   # Each block is M x d\n",
    "        order=(1, 0)\n",
    "    )\n",
    "\n",
    "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "\n",
    "    # epilogue\n",
    "\n",
    "    # Since we are dpoing everything block wise we need to have the right offset \n",
    "    # for storing m_ptrs\n",
    "    # BLOCK_SIZE + JUMP ACROSS head * num batches * partial seqlen + offset_m \n",
    "    m_ptrs = M + offset_bh * N_CTX + offs_m\n",
    "    tl.store(pointer=m_ptrs, value=m_i) # store the m_i block. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BWD KERNEL\n",
    "\n",
    "#### PASS SETUP & LAUNCH\n",
    "1. Asserts: Check if the shapes of Q, K, V tensors are good. Check if values are the type and shape they should be\n",
    "2. Allocate Input and output buffers.\n",
    "3. Decide how to parallelize the workload: grids, warps, compute units, numblocks, block size w.r.t the workload under consideration\n",
    "4. Launch FWD Triton Kernel using the `grid[]()`\n",
    "5. Save CTX needed for BWD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Tuning\n",
    "**Tile Size**\n",
    "1. use a smaller tile size (16x16) for latency sensitive workloads. Use bigger for throughput sensitive. \n",
    "2. Smaller SMEM => Smaller tile size to fit in the fused op\n",
    "3. Higher accuracy due to accumulation over smaller ranges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "Compare across input types and sizes. \n",
    "\n",
    "1. GFLOPS per second\n",
    "2. Data movement: GBPS per second\n",
    "3. wall clock time (milliseconds)\n",
    "4. GPU Level Metrics across compute and memory hierarchy\n",
    "    * Compute\n",
    "    * How many warps are active out of all the max warms - Occupancy\n",
    "    * Thread Divergence\n",
    "    * Cache hit rate\n",
    "    * Register spills\n",
    "    * Benchmarking: How many simulatenous blocks can run. What is the right balance for blocks vs tile size ? Does that/should that way across input sizes and types. large matrix small matrix, fp8, fp16, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 2x3 tensor\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                      [4, 5, 6]])\n",
    "\n",
    "print(tensor.stride())  # Output: (3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.t().is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6748, -0.5215,  0.2344,  0.1970,  0.8140,  0.1179,  0.8569, -0.4915],\n",
       "        [ 0.7129,  0.1702,  0.3752,  2.8984,  0.3518, -0.6851, -0.8940, -0.2593],\n",
       "        [ 2.7637,  0.2206,  0.1458,  0.9351, -0.4763, -1.6064, -1.3652,  1.0771],\n",
       "        [ 2.1992,  1.9307,  0.5269, -0.9233, -1.5283,  0.2676,  1.1191,  0.3557],\n",
       "        [ 0.2500,  0.8125,  1.2207,  0.3054,  0.1581, -1.5156, -0.3792, -0.8438],\n",
       "        [ 1.6123,  0.8076,  0.9468,  1.3760,  0.8350, -0.5679, -2.3848, -0.0652],\n",
       "        [ 0.6567, -0.7183,  0.7964,  0.4209,  0.6030, -0.2683, -1.1074, -2.1270],\n",
       "        [-0.3342,  0.1781,  0.8110, -0.1443, -0.7310, -1.0801,  0.3257, -0.2145],\n",
       "        [-0.9170,  0.0278, -0.5083,  1.4609,  0.9805,  0.1976, -0.4873,  0.1455],\n",
       "        [ 0.4543,  0.8237, -0.6030,  0.0179,  1.4775, -0.0371,  0.9585,  0.7080],\n",
       "        [ 0.0955,  2.5449,  1.0361, -0.6123,  0.5068,  1.8047,  0.2053, -1.0400],\n",
       "        [ 0.6323, -1.9512,  0.8726, -1.2910, -0.1454, -0.1803,  0.2067, -1.3242],\n",
       "        [-1.6699,  0.4280, -0.2128,  1.2129, -0.0694,  0.2000,  1.1738, -0.2507],\n",
       "        [ 1.4404, -1.1309,  0.0350, -0.1742, -2.7871,  1.7510, -0.5742, -0.1373],\n",
       "        [ 0.9629, -0.3103,  1.6621, -1.3740, -1.0459,  2.0254,  0.4448,  1.4521],\n",
       "        [-0.5630, -1.0996, -1.1943,  1.0732,  1.1914, -0.5049, -0.9453, -0.4475]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "N = 16\n",
    "d = 8\n",
    "a = torch.randn((N, d), dtype=torch.float16)\n",
    "block_size_N = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary of Terms\n",
    "1. Compute Bound vs Memory Bound\n",
    "2. Arithmetic Intensity \n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
