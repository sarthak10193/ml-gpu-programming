{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention Deepdive\n",
    "\n",
    "## Overview\n",
    "Flash attention has evolved over the last 3 years with v1, v2 and v3. A table with the key differences is provided at the end of this notebook. \n",
    "- Analysis and Limitations of Vanilla attention\n",
    "- Softmax and Online Softamx\n",
    "- More on \n",
    "- Vanilla attention deepdive\n",
    "- Flash Attention\n",
    "\n",
    "Discuss Advance topics\n",
    "- flex attention \n",
    "- deepseek attention methods\n",
    "- Ohter customization from character ai etc. \n",
    "\n",
    "### Why Flash Attention?\n",
    "* Vanilla attention is $O(N^2)$ in runtime and memory. As the need for larger and larger seq lenghts grows, this quadratic relationship is a big bottleneck\n",
    "* Prior attempts have mostly targeted at ideas like FLOPS reduction (sparse attention) using modified versions of attention without taking into account SW-HW co-design.\n",
    "    * HBM is an order of magnitude slower than SMEM. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How\n",
    "Flash Attention V1 is inspired by the `DataMovement is all you need` paper. This paper analyzes the data movement and storage patterns of attention ops. The key takeaway is that vanilla attention is memory-bound (figure 1). \n",
    "\n",
    "* **IO Aware**: Takes into account the GPU memory hierarchy and compute architecture. Basically move beyond the mathematical / PyTorch formulation of attention and think critically about the target devices (Mi300x, H100, H200 etc) memory and compute properties. \n",
    "    * **Tiling**: To take full advantage of the various memory capacities and bandwidth (HBM, SMEM, L2 Cache, Registers, etc), the attention algorithm is re-organized in a way that doesn't change the end result (ie exact). It uses \"math tricks\" to make the algo tiling compatible. **NOTE:** By default the attention function is not \"tilable\" due to the non-associate and non-distributive nature of the softmax function. More on this later. \n",
    "\n",
    "* **Activation Recomputation**\n",
    "How do you get here ? Using the data movement paper + facts/research observations on Elementiwise vs matmul parts in terms of memory bound. \n",
    "\n",
    "\n",
    "In simpler terms, the flash attention algo provides a **fused kernel implememtation** of the original attention function $$O = \\text{dropout(mask(softmax}(Q \\dot K^T))V))$$. \n",
    "\n",
    "Formally, Dao et al. introduced FlashAttention, a novel tiling strategy for parallelizing attention that eliminates intermediate\n",
    "reads/writes to slow global memory through fusing all of the attention operations into a single GPU kernel. Dao\n",
    "[15] restructured the algorithm as FlashAttention-2 to also parallelize over the sequence length dimension and\n",
    "perform the inner loop of the forward pass over blocks of the key and value matrices, thus improving the occupancy\n",
    "and distribution of work on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome \n",
    "As a result of Flash attention 1\n",
    "* Wall clock time speedup of 2- 4x over optimized baselines \n",
    "* O(N) in memory \n",
    "* About 25-50% FLOPS utilization (compared to theoretical max) over a mere 10% FLOPS utilization with vanilla attention. Note that this flops util is still low if you compare it to GeMMs. We'll address some of this in **Flash Attention v2**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisit GPU Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Memory Hierarchy\n",
    "| Memory Level         | H100                   | MI300X                 | Description                                                                                  |\n",
    "|-----------------------|-------------------------|-------------------------|----------------------------------------------------------------------------------------------|\n",
    "| Registers            | 256KB per SM, ~90 TB/s  | 256KB per CU, ~100 TB/s| Fastest memory, private to each thread. Stores variables and intermediate values for execution.|\n",
    "| Shared Memory (SMEM) | upto 258KB per SM, ~50 TB/s  | 256KB per CU, ~60 TB/s | Programmable memory local to thread blocks. Used for data sharing and reuse within a block. Nvidia combines the SMEM and L1 cache into a unified memory region that the programmer can control allocation ratio for. For applications with high thread-level data sharing (e.g., matrix multiplication), the GPU can allocate more of this 256 KB as shared memory. |\n",
    "| L1 Data Cache             | 128KB per SM, ~50 TB/s  | 128KB per CU, ~60 TB/s | Automatically caches frequently accessed local and global memory. Shared within an SM/CU.    |\n",
    "| L2 Cache             | 50MB, 37.5 TB/s         | 256MB for 8 XCDs, 50 TB/s          | Last-level cache before accessing HBM. Shared across all SMs/CUs on the GPU. Also called as AMD Infinity Cache.               |\n",
    "| HBM3                 | 80GB, 3.35 TB/s         | 192GB, 5.3 TB/s        | High Bandwidth Memory used for storing large datasets and models. Accessible by all SMs/CUs.|\n",
    "\n",
    "* We'll take into account the realization that HBM r/w (2TB / sec) are an order of magnitude slower than SMEM r/w (20TB / sec)\n",
    "\n",
    "**GPU Compute Hierchy**\n",
    "* Threads: Each thread has access to private set of registers. Add numbers\n",
    "* Warps: A group of 32 threads. On the AMD side a warp also called an exectuion unit (EU) is a group of 64 threads\n",
    "* Warpgroups: A group of 4 contingous warps. This is the one that as WGMMA instruction in H100s\n",
    "    * [https://pytorch.org/blog/warp-specialization/#:~:text=Warp%20specialization%20(WS)%20is%20a,task%20differentiation%20or%20cooperative%20processing][warp specialization] \n",
    "* Threadblocks (a.k.a CTA - co-operative thread arrays):\n",
    "   * WHen a block is assigned to an SM, it is divided into warps. Say we have a 2D blocks sizse 32*128 then total threads = 4096. Given a warp size of 64 on AMD we get a total of 4096/64 = 64 warps on this SM.\n",
    "   * The H100 supports a maximum of 1024 threads per block and up to 32 blocks per SM.\n",
    "* ThreadBlock Clusters \n",
    "* WARPS PER SM =\n",
    "\n",
    "When a warp encounters an instruction where an operand isn't ready (e.g., waiting for memory access), the scheduler can perform \"context switching\" to execute another warp instead of waiting idle, which helps hide memory latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compute Hierarchy and Memory Heiarchies are closely inter-twinded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Attention \n",
    "### Formulation\n",
    "Remember that the IO unaware formulation of attention is as follows:\n",
    "\n",
    "$\\text{Let } Q, K, V \\in \\mathbb{R}^{N \\times d} \\text{ be the query, key and value input sequences}$\n",
    "$ \\text{associated to a single head, where } N \\text{ is the sequence length and } d \\text{ is the} $\n",
    "$ \\text{head dimension. Then the attention output } O \\text{ is computed as:} $ \n",
    "\n",
    "\\begin{align*}\n",
    "& S = \\alpha QK^\\top \\in \\mathbb{R}^{N \\times N}, && \\\\ \n",
    "& P = \\text{softmax}(S) \\in \\mathbb{R}^{N \\times N}, && \\\\ \n",
    "& O = PV \\in \\mathbb{R}^{N \\times d}, &&\n",
    "\\end{align*}\n",
    "\n",
    "Then give B Batches and H heads (MHA) for each of Q, K, V we get the following shapes: \n",
    "* Q: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "* K: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "* V: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "\n",
    "You can parallelize the GeMM across BATCH and HEAD dimensions. \n",
    "\n",
    "\n",
    "### Limitation\n",
    "\n",
    "#### I - Attention is $O (N^2)$ in Memory Complexity \n",
    "**Materiallization**: if N is too large then it would **NOT be possible** at all to fit the entire $Q \\dot K^T$ in the `SMEM`. Since $Q \\dot K^T \\in N, N$, this means that Attention is $O(N^2)$ in memory complexity, where N is the number of tokens/sequence len.\n",
    "\n",
    "* N = 8192.\n",
    "* B = 1\n",
    "* d = 128 [does not matter in this calclulation]\n",
    "* NUM HEADS = 16\n",
    "* Precision = bfp16 (2 bytes)\n",
    "* Then NxN matrix = 8192 X 8192 X 1 X 16 elements = 1073741824 where each element takes 2 bytes.\n",
    "* <mark>Total memory required if NxN is materialized ~2148 MB. </mark>. This is TOO large to fit in SMEM which is usually in KBs.\n",
    "\n",
    "<u>Understanding the Space & Time Complexity of Attention<u>\n",
    "* Lower Bounds on IO access\n",
    "* Parameterization\n",
    "\n",
    "#### II -  Attetion is IO Bound  \n",
    "**IO Bound**:  Standard computation would require 6 trips (3 loads and 3 stores) from the HBM and is memory bound. \n",
    "* Load $Q$ & $K$   --> compute $X = Q \\dot K^T$   --> Store $X$     \n",
    "* Load $X$         --> compute $A = softmax(X)$   --> Store $A$\n",
    "* Load $A$, $V$    --> compute $O = X  V$          --> Store $O$\n",
    "\n",
    "We already know that HBM access is 10x slower than SMEM access. \n",
    "\n",
    "Add the distribution of time vs flops chart hereâ€”link to glossary.\n",
    "\n",
    "### Addressing the Limiations\n",
    "1. Tiling\n",
    "Matrix Mul Op have the nice property of **Associativity** & **Distributivity**. Therefore the $Q \\dot K^T$ can be tiled achieving massive paralllism on GPUs. \n",
    "2. However, the subsequent operation `SOFTMAX` CANNOT be tiled. Additionally, using the 'Safe Softmax' operations requires us to scan the entire sequence length for the max value. We'll use online softmax to make softmax tilable and use activation recomputation. Doing so we'll be able to compute the entire Attention OP without materialization the intermediate S and P matrices on the HBM.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Left: The GPU is a hierarchical computing and memory system. \n",
    "    * The HBM BW (gbps / seconds) is an order of magnitude (10x) slower that the the SMEM\n",
    "    * However, the the SMEM memory is 100x smaller than the HBM in storage (HBM: 80GB H100, 192 GB MI300x vs SMEM: )\n",
    "2. Middle: This is the core logic / kernel `_attention_fwd` of the flash attention algorithms in terms. This helps avoid the materialization of the N x N matrix \n",
    "    * The **outer for loop** loads the \n",
    "    \n",
    "<img src=\"diagrams/inner_loop_attn_paper.png\" alt=\"Description\" width=\"1000\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online Softmax\n",
    "**Safe Softmax** Additionally in the real world we use  `Safe Softmax` operation. Safe Softmax **subtracts the MAX values to avoid overflow  and underflow** given a certain bit-width. Ex: IN fp16 (2bytes) the max value can only be 65556. Therefore if $x_{i}=12$ then  $e^{12}$ is too large to fit into the dynamic range of FP16. \n",
    "\n",
    "* fp16 is with: `1 signed bit |  5 exponent bits |  10 mantissa` therefore very high precision but lower dynamic range \n",
    "* bf16 is with: `1 signed bit |  8 exponent bits | 7 mantissa bit`  therefore very low precision but higher dynamic range\n",
    "\n",
    "$$\\text{Safe Softmax}(x_i) = \\frac{e^{x_i - M}}{\\sum_{j=1}^{n} e^{x_j - M}} \\quad \\text{where } M = \\text{max} (x_i)$$\n",
    "\n",
    "\n",
    "The online softmax (by Milakov et al) title `Online normalizer calculation for softmax` was introduced by Nvidia in 2018. \n",
    "\n",
    "1. Take care of overflows / underflow by subtracing the max value. IF the bit width is 16 (bf16, fp16).\n",
    "2. \n",
    "\n",
    "The **implication / advantage of doing Online Softmax** is that equation (7) and equation (8) can be fused since you don't require $m_{N}$ any more and only require $m_{i}$ and $m_{i-1}$. All of this is hinged on the fact that $d_{N} == d^{`}_{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the space-time complexity of Attention \n",
    "* Lower Bounds on IO access\n",
    "* Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization: 1.0101739810710686\n",
      "safe_softmax: [4.494268374874213e-05, 0.006670085673698849, 0.9899284863184842, 0.0009026979338625064, 0.0024537873902059763]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "m = - math.inf\n",
    "l = 0\n",
    "\n",
    "a = [1, 6, 11, 4, 5]\n",
    "m = max(a)                                    # O(N)\n",
    "\n",
    "# caculation the normalization (denomintor)\n",
    "for x in a:                                   # O(N)\n",
    "    l += math.exp(x-max_a)\n",
    "print(f\"normalization: {normalization}\")\n",
    "\n",
    "safe_softmax = []\n",
    "for x in a:                                   # O(N)  \n",
    "    prob = math.exp(x-max_a) / l\n",
    "    safe_softmax.append(prob)\n",
    "    \n",
    "print(f\"safe_softmax: {safe_softmax}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile Programming\n",
    "As shown above using the **online softmax** provides us with **eventual consistency**. Therefore we can now parallelize across \n",
    "\n",
    "* X: seq length\n",
    "* Y: batch*head\n",
    "* Z: 1 [no parallelism]\n",
    "\n",
    "<mark>Each **CUDA THREAD BLOCK** gets a **UNIQUE PROGRAM ID**<mark>\n",
    "\n",
    "<img src=\"diagrams/fa_triton_program_id_mapping.png\" alt=\"Description\" width=\"700\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Attention \"Tilable\" - FWD PASS MATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "BLOCK_M: Seq Tile size\n",
    "BLOCK_N\n",
    "\n",
    "\n",
    "Block level partial softmax is computed. For the partial softmax to converge to actual softmax we need to maintain 2 statistics - `m(x)`. We do this recursively over all the N/BLOCK_SIZE number of blocks,  accumulatating the results in the accumulator O.\n",
    "\n",
    "**Notatation**\n",
    "1. Given a shared memory size of $M$ we determined the block size to be  \n",
    "\n",
    "    BLOCK_SIZE = ($B_c$,   $B_r$) where $B_c = \\left\\lceil\\frac{M}{4d}\\right\\rceil$ and $B_r = \\min\\left(\\left\\lceil\\frac{M}{4d}\\right\\rceil, d\\right) $\n",
    "\n",
    "   We'll tune the block_size and find the best M based on our GPU target arch.\n",
    "\n",
    "3. Init Tensors\n",
    "    * **Output tensor**: Allocate 0s for $O$ where $O = PV \\in \\mathbb{R}^{N \\times d}$\n",
    "    * m, l\n",
    "4. Next let's chunk the Q, K, and V matrices into block\n",
    "    * **row wise split**: divide the Q matrix into chunks ($Q_1, Q_2, .... Q_{Tr}$) where each chunk $Q_i$ is $B_r$ X $d$ size. We have a total of $T_r$ such chunks where $T_r = N / B_r$\n",
    "    * **col wise split**: divide the K and V matrix into chunks ($K_1, K_2, .... K_{Tc}$) where each chunk $K_i$ is $B_c$ X $d$ size. We have a total of $T_c$ such chunks where $T_c = N / B_c$\n",
    "\n",
    "5. Block wise output $O$ \n",
    "\n",
    "6. Implement **Fused Kernel** using Tiling and Online Softmax \n",
    "    * **Outer Loop**: Load $K_j$ and $V_j$ for each j in [1 .... $Tc$]\n",
    "    * **Inner Loop**: Load $Q_i$ for each i in [1 ... $Tr$]\n",
    "     \n",
    "\n",
    "<img src=\"diagrams/flash_attention_algo_v1.png\" alt=\"Description\" width=\"900\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Let } Q, K, V \\in \\mathbb{R}^{N \\times d} \\text{ be the query, key and value input sequences}$\n",
    "$ \\text{associated to a single head, where } N \\text{ is the sequence length and } d \\text{ is the} $\n",
    "$ \\text{head dimension. Then the attention output } O \\text{ is computed as:} $ \n",
    "\n",
    "\\begin{align*}\n",
    "& S = \\alpha QK^\\top \\in \\mathbb{R}^{N \\times N}, && \\\\ \n",
    "& P = \\text{softmax}(S) \\in \\mathbb{R}^{N \\times N}, && \\\\ \n",
    "& O = PV \\in \\mathbb{R}^{N \\times d}, &&\n",
    "\\end{align*}\n",
    "\n",
    "Then give B Batches and H heads (MHA) for each of Q, K, V we get the following shapes: \n",
    "* Q: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "* K: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "* V: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "\n",
    "1. Batches and the heads are to be treated independently [heads don't pass around info]. Therefore, the effective matrix becomes `[B*H, N, d]`where `B*H` acts as the batch dimension. In addition, **Flash Attention 2** parallelizes along the sequence dimension as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Flash attention v1** paralellizes over the batch size and the number of heads since this each head is indepdendent. 1 threadblock is used to process 1 attention head.  Therefore given a `batch size=B` and `number of heads=H`, we have **total number of threadblocks being launched = H * B**\n",
    "* `A100 has 108SM SM`,  `H100 has 114SMs` and `Mi300x has 304SMs`. Therefore, the above scheduling of a) parallelization across heads b) 1 head per SM is effciiency when Total heads >= 100 since we end up using most compute resources of the GPU.\n",
    "* However, in the case of long sequences (Llama2 8192) we usually have to go with both smaller batch sizes and less number of heads. Therefore this scehduling becomes inefficient. We'll fix this in **FlashAttention 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"diagrams/inner_loop_attn.png\" alt=\"Description\" width=\"500\" height=\"300\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Store and Load\n",
    "* Transpose\n",
    "* Materialization\n",
    "* Load Store Units, Vectorized Load Stores\n",
    "* Strides, Contingous Memory , Non contiguous MEmory\n",
    "* Moving data from HBM to SMEM. Inner workings of SMEM\n",
    "* L1 cache, L2 cache, REgisters\n",
    "\n",
    "Explain how is Q actually stored in memory ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Flash Attention FWD\n",
    "* Flash Attention 2 FWD Math\n",
    "\n",
    "### Feature Support List\n",
    "* Causal Masking\n",
    "* Dropout\n",
    "    * Requires a random number generator per thread. Philox Seed \n",
    "    * Modern LLMs like LLama2, 3 do not use attention dropout. Earlier Encoder only models like BERT do use attention dropout\n",
    "* Attention Bias\n",
    "* Attention Variants\n",
    "    * Multi-Head Attention\n",
    "    * Grouped Query Attention: Group query attention helps reduce the KV cache size by sharing K, V across a set of Queries. This require index manipulation to make the core attention $O= \\text{softmax}(Q K^T)V$ work \n",
    "    * Multi-Query Attention\n",
    "    * Sliding Window Attention (SWA): A default value of `(-1, -1)` means attend to everything on the left and right.\n",
    "    * Ring Attention and Context parallelism\n",
    "* Sequence Length Support\n",
    "    * Variable Seq Lenghts\n",
    "    * Fixed Sequence Lengths.  \n",
    "* FP8 Support\n",
    "    * Scaling Strategies\n",
    "* More Modern Attention Variants\n",
    "  * MLA - Mult-head latent Attention\n",
    " \n",
    "\n",
    "### Python Autograd API\n",
    "This section creates the python wrapper that integrates into Pytorch by inheriting the `torch.autograd.Function` class and implementing the `foward()` and `backward()` member functions.\n",
    "\n",
    "**FWD PASS SETUP & LAUNCH**\n",
    "\n",
    "The broad recipe we follow: \n",
    "1. Asserts: Check if the shapes of Q, K, V tensors are good. Check if values are the type and shape they should be.\n",
    "2. Allocate Input and Output buffers.\n",
    "3. Decide how to parallelize the workload - reason about grids, warps, compute units, numblocks, block size w.r.t the workload under consideration\n",
    "4. Launch FWD Triton Kernel using the `kernel_name[grid](<kernel params>)`\n",
    "5. Save CTX needed for BWD. This context would not be necessary if were doing inference. The context is essentially data that the BWD pass will require. \n",
    "    - LSE\n",
    "\n",
    "\n",
    "Its key to remember that while our $X \\in mathbb{R}^{B, H, Seq, d]$\n",
    "\n",
    "the Transformer is operating at a token level. Therefore we can generalize the input to [Num Tokens, d] where Num `Tokens = B * H * Seq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "SEQ_LEN = 4096\n",
    "EMB_DIM = 1024\n",
    "\n",
    "# HEADS_Q == HEADS_KV is for Multi-head attention\n",
    "# HEADS_Q != HEADS_KV and HEADS_Q > HEADS_KV is for MQA\n",
    "NUM_HEADS_Q = 16\n",
    "NUM_HEAD_KV = 16\n",
    "\n",
    "device = torch\n",
    "\n",
    "\n",
    "# Add support for QKV, QK and V, Q and K and V\n",
    "flash_attention_v2(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_heads_q=NUM_HEADS_Q, \n",
    "    num_heads_kv=NUM_HEAD_KV,\n",
    "    seq_len=SEQ_LEN,\n",
    "    emb_dim=EMB_DIM,\n",
    "    causal=True,\n",
    "    mode='fwd',\n",
    "    dropout=False\n",
    ")\n",
    "\n",
    "# python api\n",
    "def flash_attention_v2(\n",
    "    batch_size: int,\n",
    "    num_heads_q: int,\n",
    "    num_heads_kv: int, \n",
    "    seq_len: int,\n",
    "    emb_dim: int,\n",
    "    causal:bool,\n",
    "    mode: str,\n",
    "    dropout: bool\n",
    "):\n",
    "    dtype = torch.float16\n",
    "\n",
    "    # Flash Attention 1: Parallelizes over Batch and Heads\n",
    "    # Flash Attention 2: Parallelises over Batch, Heads and Sequence Len\n",
    "    q = torch.randn((batch_size, num_heads_q, seq_len, emb_dim), dtype=dtype, device=device) \n",
    "    k = torch.randn((batch_size, num_heads_kv, seq_len, emb_dim), dtype=dtype, device=device) \n",
    "    v = torch.randn((batch_size, num_heads_kv, seq_len, emb_dim), dtype=dtype, device=device) \n",
    "    \n",
    "    if mode == 'fwd':\n",
    "        q = q.to(torch.float8_e5m2)  # why e5m2 for FWD pass ?\n",
    "        k = k.to(torch.float8_e5m2)  # what is _nuz ?\n",
    "\n",
    "    softmax_scale = 1 / math.sqrt(EMB_DIM)  \n",
    "    fn = lambda: FusedAttention.apply(q, k, v, causal, softmax_scale, dropout)\n",
    "    ms = triton.testing.do_bench(fn) # benchmark the op and report time in milli seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m \n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_hip\u001b[39m():\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'triton'"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "import torch \n",
    "\n",
    "def is_hip():\n",
    "    return True if triton.runtime.driver.active.get_current_target().backend == 'hip' else False\n",
    "\n",
    "\n",
    "class FusedAttention(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, v, causal:bool, softmax_scale: float):\n",
    "        \"\"\"\n",
    "        the ctx object is used to build ctx that is required during the bwd pass\n",
    "        we'll use ctx.save_for_backward(...) to do this. \n",
    "        \"\"\"\n",
    "        ### ------ 1. Check Inputs --------------\n",
    "        HEAD_EMB_DIM_Q = HEAD_EMB_DIM_K = q.shape[-1] = k.shape[-1]  # q: [B, H, S, d]\n",
    "        HEAD_EMB_DIM_V = v.shape[-1]\n",
    "\n",
    "        assert HEAD_EMB_DIM_Q == HEAD_EMB_DIM_K == HEAD_EMB_DIM_V, \"All heads must have the same embedding dimension\"\n",
    "        assert HEAD_EMB_DIM_K in {16, 32, 64, 128, 256}, \"Only head size of 16, 32, 65, 128, 256 are supported\"  \n",
    "        # why are only these supported ??\n",
    "        # - usually we prefer powers of 2 because of GPU SMs (ALUs)being a power of 2 for distirbuting computation. \n",
    "        # -  \n",
    "\n",
    "        ### ------- 2. Allocate Buffers ----------\n",
    "        o = torch.empty_like(q)      # [B, H, Seq, d]\n",
    "        stage = 3 if causal else 1   # causal requires special considerations such as masking. If masking is required then additional memory needs to be allocated for the boolean mask. \n",
    "        \n",
    "        ### ------ 3. Grid, Warps, EUs, Blocks etc ---------\n",
    "        # SM == EU; \n",
    "        # WARPS (32 threads) == WAVEFRONTS (64 threads) ; \n",
    "        # OCCUPANCY == WAVES_PER_EU == ACTIVE WARPS PER SM\n",
    "        # How does this affect perf\n",
    "            # - register pressure \n",
    "        if is_hip(): \n",
    "            # Increase the active warps count if ...\n",
    "            # Decrease the warp count if ...\n",
    "            waves_per_eu = 3 if HEAD_EMB_DIM_K <= 64 else 2 \n",
    "            extra_kern_args = {\"waves_per_eu\": waves_per_eu, \"allow_flush_denorm\": True}\n",
    "\n",
    "        num_heads_q = q.shape[1]\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        # BLOCK_M is B_r\n",
    "        # If seq_len=4096 and BLOCK_M=128 then NUM_BLOCKS=32. Each Threadblock will process BLOCK_M elements of the seq.\n",
    "        x_grid = triton.cdiv(q.shape[2], BLOCK_M)  \n",
    "        y_grid = num_heads_q * batch_size    # HEADS and BATCH_SIZE are truly independent. No TB communication required for parallelization.  \n",
    "        z_grid = 1 \n",
    "        grid = (x_grid, y_grid, z_grid) # parallelization was done from inwards to outwards. \n",
    "        \n",
    "        mask = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n",
    "\n",
    "        ### ------ 4. Launch ---------\n",
    "        _attn_fwd[grid](\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            softmax_scale,  # softmax scale. \n",
    "            mask,           # causal mask tensor\n",
    "            o,              # Final Output  \n",
    "            dropout,        # dropout\n",
    "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access. \n",
    "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access.\n",
    "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access.\n",
    "            o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access.\n",
    "            batch_size,              # Batch size\n",
    "            num_heads_q,             # Number of query heads\n",
    "            N_CTX=q.shape[2],        # SeqLen\n",
    "            HEAD_DIM=HEAD_DIM_K,     #\n",
    "            STAGE=stage,  \n",
    "            **extra_kern_args\n",
    "        )\n",
    "\n",
    "        ### Save context for BWD\n",
    "        ctx.save_for_backward(q, k, v, o, M)\n",
    "        ctx.grid = grid\n",
    "        ctx.sm_scale = sm_scale       # normalization factro diag(l)^-1\n",
    "        ctx.HEAD_DIM = HEAD_DIM_K\n",
    "        ctx.causal = causal\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward():\n",
    "        \"\"\"\n",
    "        You can make mistakes while implemetning the bwd pass. To help with this we can use the .gradcheck() to use \n",
    "        infinite small internal [f(a+h) - f(a-h)] / 2h. In addition to that people use tools like PySolver.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FWD KERNEL in TRITON\n",
    "\n",
    "Given:\n",
    "* S = 8192\n",
    "* BLOCK_M = 64\n",
    "* d = 128\n",
    "* NUM_HEADS (H) = 32\n",
    "* BATCH_SIZE (B = 8\n",
    "\n",
    "grid_x = NUM_BLOCKS_IN_SEQ_DIM = ceil_div(S/BLOCK_M) = 8192 / 64 = 128. # We'll process 64 tokens at a time\n",
    "grid_y = B*H # easy parallilism along Batch and Head Dim as done in flash attention v1. \n",
    "\n",
    "We can think about the parallelism as \n",
    "* Pure Parallisim: Where the task is easily parallilization no ifs and buts.\n",
    "* Tiling: GPU programming required, warps etc. \n",
    "\n",
    "A column is a full sequence for a given batch and head. Each column is chunked into block sizes of size 64, s.t that there are a total of 128 chunks.  \n",
    "**Example 1 (3, 12)**\n",
    "- `start_m = program_id(3)`       # processing **the 4th chunk** of the seq [0-63, 64-127, 128-195, **196-255(4th chunk)**, .... 8128-8191]\n",
    "- `offset_bh = program_id(12)`\n",
    "- `offset_bh = 12`\n",
    "- `offset_bh // H` = 12 // 32 =  0th Batch\n",
    "- `offset_bh  % H` = 12 % 32  = 12th Head\n",
    "\n",
    "\n",
    "**Example 2 (*, 50)**\n",
    "- For a given sequence chunk we current process \n",
    "    - offset_bh = 50\n",
    "    - offset_bh // H = 50 // 32 =  1st Batch\n",
    "    - offset_bh  % H = 50 % 32  = 18th Head\n",
    "\n",
    "\n",
    "**Example 3 (127, 120)**\n",
    "- start_m = 127      # Last Sequence chunk [8128-8191]\n",
    "- offset_bh = 120\n",
    "- offset_bh // H = 120 // 32 = 7\n",
    "- offset_bh % H = % 32 = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (132727422.py, line 84)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31moffsets = (start_m * BLOCK_M, 0)   # offsets to the block ==>  how to access the next block\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def _attn_fwd_inner(\n",
    "    acc, \n",
    "    li, \n",
    "    mi, \n",
    "    q,  # [BLOCK_M, HEAD_DIM]\n",
    "    K_block_ptr, V_block_ptr, \n",
    "    start_m, \n",
    "    qk_scale,  # softmax scale, default to sqrt(HEAD_DIM)\n",
    "    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  #\n",
    "    STAGE: tl.constexpr, \n",
    "    offs_m: tl.constexpr, offs_n: tl.constexpr,  #\n",
    "    N_CTX: tl.constexpr,  # The operation is parallelized by dividing the sequence length (N_CTX) into blocks of size BLOCK_N.\n",
    "    fp8_v: tl.constexpr\n",
    "        \n",
    "): \n",
    "    \"\"\"\n",
    "    Load blocks of Q, K and V from HBM.\n",
    "    \"\"\"\n",
    "    if STAGE == 1:\n",
    "        lo, hi = 0, start_m * BLOCK_M\n",
    "    elif STAGE == 2:\n",
    "        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n",
    "        lo = tl.multiple_of(lo, BLOCK_M)\n",
    "    # causal = False, so keep the entire range of 0, N_CTX[no masking]\n",
    "    else:\n",
    "        lo, hi = 0, N_CTX\n",
    "    \n",
    "    # low, high marks the low/high index position over seq len. \n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _attention_fwd(\n",
    "    Q,                       # Q:  M, K matrix\n",
    "    K,                       # K:  M, K matrix\n",
    "    V,                       # V:  M, K matrix\n",
    "    softmax_scale, \n",
    "    OUT,                     # O:  M, K matrix\n",
    "    stride_qb, stride_qh, stride_qm, stride_qk,  # jump next batch, jump next head, jump next seq, jump next d  \n",
    "    stride_kb, stride_kh, stride_kn, stride_kk,  # jump next batch, jump next head, jump next seq, jump next d  \n",
    "    stride_vb, stride_vh, stride_vk, stride_vn,  # jump next batch, jump next head, jump next seq, jump next d  \n",
    "    stride_ob, stride_oh, stride_om, stride_on,  # jump next batch, jump next head, jump next seq, jump next d  \n",
    "    B,                       # Batch Size \n",
    "    H,                       # Number of Heads\n",
    "    N_CTX,                   # Sequence Length: 8192\n",
    "    lse,                     # Log Sum Exponential. Required for BWD. \n",
    "    dropout_p,               # Attention Dropout \n",
    "    is_causal,               # Causal Attention Flag\n",
    "    HEAD_DIM: tl.constexpr,  # d\n",
    "    BLOCK_M: tl.constexpr,   # Tile size in the QUERY SEQ DIMENSION\n",
    "    BLOCK_N: tl.constexpr,   # Tile size for Key/Value dimension\n",
    "    STAGE: tl.constexpr      #git\n",
    "\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    3. Variable sequence Lengths Layout (thd) --> (token, head,  dim_per_head)\n",
    "    4. Context Parallelism\n",
    "        - Due to intern token communication during attention, we need to all gather during fwd and reduce scatter during bwd\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Here we try to create a highly parallelizable \"instance of the program\". Therefore we must\n",
    "    1. Define the program with the correct way for a program instance to know what data to work [which block, which tile, which batch which head & so on]\n",
    "    2. \n",
    "    \"\"\"\n",
    "    start_m = tl.program_id(0)    # get the current program instance along x axis. Achieve parallelism by tiling along seq dim [X-axis]\n",
    "    offset_bh = tl.program_id(1)  # get the current program instance along y axis.  Parallelism across the Batch * Head\n",
    "\n",
    "    off_b = offset_bh // H  # Current batch.  \n",
    "    off_h = offset_bh % H   # Current head in that batch \n",
    "\n",
    "    # represents the starting point of the block. \n",
    "    qkv_offset =  off_b.to(tl.int64) * stride_qb  + off_h.to(tl.int64) * stride_qh\n",
    "\n",
    "    # With very-large tensor sizes, the offsets can be huge. Use int64 to avoid overflow. \n",
    "    # Get Q given a head and batch iie qkv_offset = batch_id * stride_batch_Q + head_id * stride_head_Q. \n",
    "    # Programs gets dispatched onto the BLOCKs(The abstraction for Triton)\n",
    "    # make_block_ptr: Returns a pointer to a block in a parent tensor\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base = Q + qkv_offset,             # offset Q by qkv_offset to get to the current batch * head for the current threadblock. \n",
    "        shape = (N_CTX, HEAD_DIM),         # (N, d)\n",
    "        strides = (stride_qm, stride_qk),  # The strides of the parent tensor [SeqLen X EmbedDim] ==> how to jump rows and cols of Q [mk] matrix. Helps with info contigous memory storage & access. \n",
    "        offsets = (start_m * BLOCK_M, 0)   # offsets to the block ==>  how to access the next block\n",
    "        block_shape = (BLOCK_M, HEAD_DIM)  # Size of the block to load from HBM to SMEM / LDS [tile size of seqlen, d]\n",
    "        order = (1, 0)                     # The order of the original data format [TN format]\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base = K + qkv_offset,             # The base pointer to the parent tensor\n",
    "        shape = (N_CTX, HEAD_DIM),         # Shape of the parent tensor (seqlen tile size , head_dim)\n",
    "        strides = (stride_kk, stride_kn),  # The strides of the parent tensor [EmbedDim X SeqLen] ==> How to jump rows and cols of K [kn] matrix \n",
    "        offsets = (0, 0)                   # Since for a given Q_i we get \"ALL\" K_j and V_j for j = 1 ....\n",
    "        block_shape = (HEAD_DIM, N_CTX)    # \n",
    "        order = (0, 1)                     # The order of the original data format\n",
    "    )\n",
    "\n",
    "    v_order: tl.constexpr = (0, 1) if V.dtype.element_ty == tl.float8e5 else (1, 0)\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base = V + qkv_offset,             # The base pointer to the parent tensor\n",
    "        shape = (N_CTX, HEAD_DIM),         # Shape of the parent tensor (seqlen tile size , head_dim)\n",
    "        strides = (stride_vk, stride_vn),  # The strides of the parent tensor [EmbedDim X SeqLen] ==> How to jump rows and cols of V [kn] matrix \n",
    "        offsets=(0, 0),                    # Since for a given Q_i we get \"ALL\" K_j and V_j for j = 1 ....\n",
    "        block_shape = (BLOCK_N, HEAD_DIM)  # SEQ TILE SIZE , HEAD DIM\n",
    "        order = v_order                    # The order of the original data format\n",
    "    )\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=OUT + qkv_offset,             # start of current block\n",
    "        shape=(N_CTX, HEAD_DIM),           # N X d\n",
    "        strides=(stride_om, stride_on),    # (Jump Seq, Jump d) \n",
    "        offsets=(start_m * BLOCK_M, 0),    # Output BLOCKS\n",
    "        block_shape=(BLOCK_M, HEAD_DIM),   # Each block is M x d\n",
    "        order=(1, 0)\n",
    "    )\n",
    "\n",
    "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "\n",
    "    # epilogue\n",
    "\n",
    "    # Since we are dpoing everything block wise we need to have the right offset \n",
    "    # for storing m_ptrs\n",
    "    # BLOCK_SIZE + JUMP ACROSS head * num batches * partial seqlen + offset_m \n",
    "    m_ptrs = M + offset_bh * N_CTX + offs_m\n",
    "    tl.store(pointer=m_ptrs, value=m_i) # store the m_i block. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FWD Pass Correctness Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BWD KERNEL\n",
    "### BWD Pass Correcntess Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Tuning\n",
    "**Tile Size**\n",
    "1. use a smaller tile size (16x16) for latency sensitive workloads. Use bigger for throughput sensitive. \n",
    "2. Smaller SMEM => Smaller tile size to fit in the fused op\n",
    "3. Higher accuracy due to accumulation over smaller ranges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "Compare across input types and sizes. \n",
    "\n",
    "1. GFLOPS per second\n",
    "2. Data movement: GBPS per second\n",
    "3. wall clock time (milliseconds)\n",
    "4. GPU Level Metrics across compute and memory hierarchy\n",
    "    * Compute\n",
    "    * How many warps are active out of all the max warms - Occupancy\n",
    "    * Thread Divergence\n",
    "    * Cache hit rate\n",
    "    * Register spills\n",
    "    * Benchmarking: How many simulatenous blocks can run. What is the right balance for blocks vs tile size ? Does that/should that way across input sizes and types. large matrix small matrix, fp8, fp16, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the TriDao Repo in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary of Terms\n",
    "1. Compute Bound vs Memory Bound\n",
    "2. Arithmetic Intensity \n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
