{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention Deepdive\n",
    "\n",
    "## Overview\n",
    "Flash attention has evolved over the last 3 years with v1, v2 and v3. A table with the key differences is provided at the end of this notebook. \n",
    "- Analysis and Limitations of Vanilla attention\n",
    "- Softmax and Online Softamx\n",
    "- More on \n",
    "- Vanilla attention deepdive\n",
    "- Flash Attention\n",
    "\n",
    "Dao et al. introduced FlashAttention, a novel tiling strategy for parallelizing attention that eliminates intermediate\n",
    "reads/writes to slow global memory through fusing all of the attention operations into a single GPU kernel. Dao\n",
    "[15] restructured the algorithm as FlashAttention-2 to also parallelize over the sequence length dimension and\n",
    "perform the inner loop of the forward pass over blocks of the key and value matrices, thus improving the occupancy\n",
    "and distribution of work on the GPU\n",
    "\n",
    "\n",
    "\n",
    "In more simpler terms, the flash attention algo provides a fused kernel implememtation of the original attention function $O = dropoutmask(softmax(Q \\dot K^T))V)$. \n",
    "\n",
    "### Why\n",
    "* Vanilla attentoin is O(N2) ... Elaborated in detail below in the section. \n",
    "* Prior attempts have mostly targeted at ideas like FLOPS reduction using modified sparse versions of attention without taking into account the SW-HW co-design. \n",
    "\n",
    "### How\n",
    "\n",
    "* **IO Aware**: Takes into account the GPU memory hierarchy and compute architecture. \n",
    "    * **Tiling**: To take full advantage of the differences in diff memory capacities and bandwidth,  the attention algorithm is re-organized in a way that doesn't change the end result (ie exact). The re-org basically help us realize helps with being able to tile\n",
    "    * **Activation Recomputation**\n",
    "How do you arrive here ? Using the data movement paper + facts / reserach observations on Elementiwise vs matmul parts in terms of being memory bound. \n",
    "\n",
    "* **Exact**: The flash attention algo \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisit GPU Fundamentals\n",
    "[![](https://mermaid.ink/img/pako:eNqdWH9vGzcS_SqEihYJ4D15JVlw9ccBluzYQayzESlX4E6tQe9SEs-73D2Sq0Rtms9-b8jdFVe1015SoFkNOZzHmTc_mN96SZGK3qS30bzcsuXlSjH8MdWjF8xFXuj9v1c9_8FupNBcJ9s9-4HNtlzzxAotjZWJWfV-9sr0Zx5D5zorHnlWn8Fe3Uznr1crNTo9jc5PT1myTzJhWMatUMkeC1_i8fWU2cLyDL9mBcdyIlLGk0QYM2HDwTSKB-dTWuTJVrBMKkHiPqTscW-FwdKFLXKZsKI0jO-4zPhjJiC-ms1YqQsrEisL1cU6ANbbAXOHEo7haTR6Dt_5_ABvgYMKzYz81UFozYfIQlg_aWlF9MiTJ1YWmXQHvhc8jbguKpVGH2m9C2sIWAv4GC5oXLiYXzkfAuKz_hu9m7JSaLaY4zeh4uqJrE_xN_soU7udsFGLyUndMf4K0pq--4m1fxTsay4bAdt7sUHkYe2NdCqxPytANDgbM13vMn271bgwMcCDbFYgmfNPbHx2cjYcA0gEILRo-u4arZmSp6lUmxZcB1FIPRZFf_-86n35ESy7nva7rJiwL3FlOgSzmivD3S3B4s8gREiO5rQRW_qz6tAnLtI5L0sCRWrDMHiNWvxjo9dexJQyA0M2uJHdOs2R1xQq9R9HWbh0noPL_Qe70xuu5K-cECMRb2WO0HXcsXT5p2VaOzeVOW4--GUYR_EJXH02PKv_qjc4XjMfonYn1i73ipPnSuR6lolMmhy4yrLQVqRdk4MW4DQrwPNZVrnA1xaQCyQGxbycPOnZfRt7Z1JQ_BqIq7UkV-1VEogNdqVVRkXBBRSQ9l0Qwy4I058tLxoEg9PReXPFvgPT8U58OhiduP-NR1hY7ksy4NI4OosHjSYlDulGyCfhACJLKAJyJ46CQFnyE9flNXK8JMUR-4ifhr2i0lCf9_rgCKmM1ZUjInus1mvnJDqAbegEbyysarOiQCZxMu23HAE4qwEYXw4ON7ilC8ClJROfRFK5BAeMt_Mlo_IuKekrLRrzO1QBtq5UkyOhjTFsvFWp3Mm0AoWWrQ3n8rACeLvYk_tyJtc-FQSxtKb220twIUuqjHtTK3UvdOSBM2NRP19I-6VLe7AwZKQXDUN-eNEojJIXnYV-86Lx19PyhusU8RRwQPPJ3gtTVBoNC3l51bo2hHxDqbmYN1m7UhEjT4Gajhu-6HlZPG5yphZ6_rRpoL36AAQ2qCWQUnZ5XkdU0t_cj0csKbRwJ3RQULb6ZrXYIzq5U7mNHd3fTbH9hL6mrpX5tQG6xHzav76fHS8txSdiC3qLV3XCGcJnubIT5tqSb5wdCJSrsyIvKxv4zelSesw-XF4csPvbo7EYKr2hFNfMCp72DYqyYJVqnDpiizcfmm7YsTs6zDMAaXWRta5semYbhSqzskRTqymbbLlSIvOb39_dY8hQKVvOPxyS4iW23JUWEfdFGwDCn-y6Qi0hd3Zz6y4-IL1wUxC75xappDyAi0xuFDxbz0BT9kizBEfl9OsL8d9KKCuRcn6Ioo4TqO8KmYI7O6E32FbvcUsfjGAPD5zOf3jAFk1DwM5H-5-u_2HwSZ3jjfe8eSEv74hod0lSlRxDQccL3olNV6wM3whmEUDX44Eew04txRgJ7WbBtxcavdDQj46jnueXePofNI0cF_P05ShfW-8h05h5ATIR0w1HoMcaw5p19Myq1kpDE1YEndizburnLAwGCshb8_d-eAGDk62SCIv3s7NinsTH9tJByNIUbjVfwdn0F3a1XstE1lMXWCsVeUUEsX3EjEP1wRtB92Ypt5ytXTZthW51KfKuyARNzcmtzOhIs0VjQkIEvaq-irMANZFK38K2Utk_z4sFmtpWF-qQGkcSdv98e11Qdngu3IqdyByKhwdqknWje_X6j7KHBAywz64glZ-VFxri50OwGDQhOIZAPmxPw5zihAeB2ncFYHhW2K4sp5Iebn0Bg3souCIeoEiqlF-KnUzEwZ2iPprWFhaXy59fu9qBMt2lP4viMpihD0UrkB4qV-chsfzDQ7FOsGl_PJr6qtaZz30rqqdvKpmuCDXpc3hm9f0nYlhUm-1LbwU3tHYeWbV59wgp3SSqnmp-w07CTadOuv4QHUvb14abzY4nptC8G1d99wztDy4ZNXSqAhlNTNJ6-eF1ydYC5Giu7V6Shcr2IYp2JlnLjF7pQR05iuL337N3Yo9T6qlrKzExqmZe-lv0ufaR8TUZL444fPAsx_UmJOqOu2Z-mPgWNOEZalJ-9qtvGbx6buJavZkB0Nfo1n5y-tzOb3eDet_bvAQb3FPtJvZri-YMTMQabOD0wg10F4MX1uupb75sDrjA2F0ffhd3qG73WRMkcmk2-W59Tv-doBAWT2Ly3XA4rL8j_9YelJ9C3XrE_SbdMJj_v3Zn2PimE45r8rc5IEjkv35C76SXC51zmfYmvd_ovFXPbtFaV70JPjHv4E2wUr9jH69sQUh7E_QmcdJzud-brHlm8Ksq0fPEpeQoWXmzBWPJv4oibzehe2Gcmft_FUvQ_-Wm9_v_ACbpScY?type=png)](https://mermaid.live/edit#pako:eNqdWH9vGzcS_SqEihYJ4D15JVlw9ccBluzYQayzESlX4E6tQe9SEs-73D2Sq0Rtms9-b8jdFVe1015SoFkNOZzHmTc_mN96SZGK3qS30bzcsuXlSjH8MdWjF8xFXuj9v1c9_8FupNBcJ9s9-4HNtlzzxAotjZWJWfV-9sr0Zx5D5zorHnlWn8Fe3Uznr1crNTo9jc5PT1myTzJhWMatUMkeC1_i8fWU2cLyDL9mBcdyIlLGk0QYM2HDwTSKB-dTWuTJVrBMKkHiPqTscW-FwdKFLXKZsKI0jO-4zPhjJiC-ms1YqQsrEisL1cU6ANbbAXOHEo7haTR6Dt_5_ABvgYMKzYz81UFozYfIQlg_aWlF9MiTJ1YWmXQHvhc8jbguKpVGH2m9C2sIWAv4GC5oXLiYXzkfAuKz_hu9m7JSaLaY4zeh4uqJrE_xN_soU7udsFGLyUndMf4K0pq--4m1fxTsay4bAdt7sUHkYe2NdCqxPytANDgbM13vMn271bgwMcCDbFYgmfNPbHx2cjYcA0gEILRo-u4arZmSp6lUmxZcB1FIPRZFf_-86n35ESy7nva7rJiwL3FlOgSzmivD3S3B4s8gREiO5rQRW_qz6tAnLtI5L0sCRWrDMHiNWvxjo9dexJQyA0M2uJHdOs2R1xQq9R9HWbh0noPL_Qe70xuu5K-cECMRb2WO0HXcsXT5p2VaOzeVOW4--GUYR_EJXH02PKv_qjc4XjMfonYn1i73ipPnSuR6lolMmhy4yrLQVqRdk4MW4DQrwPNZVrnA1xaQCyQGxbycPOnZfRt7Z1JQ_BqIq7UkV-1VEogNdqVVRkXBBRSQ9l0Qwy4I058tLxoEg9PReXPFvgPT8U58OhiduP-NR1hY7ksy4NI4OosHjSYlDulGyCfhACJLKAJyJ46CQFnyE9flNXK8JMUR-4ifhr2i0lCf9_rgCKmM1ZUjInus1mvnJDqAbegEbyysarOiQCZxMu23HAE4qwEYXw4ON7ilC8ClJROfRFK5BAeMt_Mlo_IuKekrLRrzO1QBtq5UkyOhjTFsvFWp3Mm0AoWWrQ3n8rACeLvYk_tyJtc-FQSxtKb220twIUuqjHtTK3UvdOSBM2NRP19I-6VLe7AwZKQXDUN-eNEojJIXnYV-86Lx19PyhusU8RRwQPPJ3gtTVBoNC3l51bo2hHxDqbmYN1m7UhEjT4Gajhu-6HlZPG5yphZ6_rRpoL36AAQ2qCWQUnZ5XkdU0t_cj0csKbRwJ3RQULb6ZrXYIzq5U7mNHd3fTbH9hL6mrpX5tQG6xHzav76fHS8txSdiC3qLV3XCGcJnubIT5tqSb5wdCJSrsyIvKxv4zelSesw-XF4csPvbo7EYKr2hFNfMCp72DYqyYJVqnDpiizcfmm7YsTs6zDMAaXWRta5semYbhSqzskRTqymbbLlSIvOb39_dY8hQKVvOPxyS4iW23JUWEfdFGwDCn-y6Qi0hd3Zz6y4-IL1wUxC75xappDyAi0xuFDxbz0BT9kizBEfl9OsL8d9KKCuRcn6Ioo4TqO8KmYI7O6E32FbvcUsfjGAPD5zOf3jAFk1DwM5H-5-u_2HwSZ3jjfe8eSEv74hod0lSlRxDQccL3olNV6wM3whmEUDX44Eew04txRgJ7WbBtxcavdDQj46jnueXePofNI0cF_P05ShfW-8h05h5ATIR0w1HoMcaw5p19Myq1kpDE1YEndizburnLAwGCshb8_d-eAGDk62SCIv3s7NinsTH9tJByNIUbjVfwdn0F3a1XstE1lMXWCsVeUUEsX3EjEP1wRtB92Ypt5ytXTZthW51KfKuyARNzcmtzOhIs0VjQkIEvaq-irMANZFK38K2Utk_z4sFmtpWF-qQGkcSdv98e11Qdngu3IqdyByKhwdqknWje_X6j7KHBAywz64glZ-VFxri50OwGDQhOIZAPmxPw5zihAeB2ncFYHhW2K4sp5Iebn0Bg3souCIeoEiqlF-KnUzEwZ2iPprWFhaXy59fu9qBMt2lP4viMpihD0UrkB4qV-chsfzDQ7FOsGl_PJr6qtaZz30rqqdvKpmuCDXpc3hm9f0nYlhUm-1LbwU3tHYeWbV59wgp3SSqnmp-w07CTadOuv4QHUvb14abzY4nptC8G1d99wztDy4ZNXSqAhlNTNJ6-eF1ydYC5Giu7V6Shcr2IYp2JlnLjF7pQR05iuL337N3Yo9T6qlrKzExqmZe-lv0ufaR8TUZL444fPAsx_UmJOqOu2Z-mPgWNOEZalJ-9qtvGbx6buJavZkB0Nfo1n5y-tzOb3eDet_bvAQb3FPtJvZri-YMTMQabOD0wg10F4MX1uupb75sDrjA2F0ffhd3qG73WRMkcmk2-W59Tv-doBAWT2Ly3XA4rL8j_9YelJ9C3XrE_SbdMJj_v3Zn2PimE45r8rc5IEjkv35C76SXC51zmfYmvd_ovFXPbtFaV70JPjHv4E2wUr9jH69sQUh7E_QmcdJzud-brHlm8Ksq0fPEpeQoWXmzBWPJv4oibzehe2Gcmft_FUvQ_-Wm9_v_ACbpScY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitation of Vanilla Attention \n",
    "\n",
    "### I - Attention is $O (N^2)$ \n",
    "1. **Memory Bound**:  Standard computation would require 6 trips (3 loads and 3 stores) from the HBM and is memory bound. \n",
    "    * Load $Q$ --> $K$ ; compute $X = Q \\dot K^T$ --> Store $X$\n",
    "    * Load $X$ --> compute $A = softmax(X) --> Store $A$\n",
    "    * Load $A$, $V$ --> compute $O = X  V$ --> Store $O$\n",
    "\n",
    "    Add the distribution of time vs flops chart here. Link to glossary. \n",
    "\n",
    "2. **Materiallization**: If L is too large (for say a million context length) then it would **NOT be possible** at all to fit $Q \\dot K^T$ in the `SMEM`.\n",
    "3. **Safe Softmax** Additionally in the real world we use  `Safe Softmax` operation. Safe Softmax **subtracts the MAX values to avoid overflow  and underflow** given a certain bit-width. Ex: IN fp16 (2bytes) the max value can only be 65556. Therefore if $x_{i}=12$ then  $e^{12}$ is too large to fit into the dynamic range of FP16. \n",
    "    * fp16 is with: `1 signed bit |  5 exponent bits |  10 mantissa` therefore very high precision but lower dynamic range \n",
    "    * bf16 is with: `1 signed bit |  8 exponent bits | 7 mantissa bit`  therefore very low precision but higher dynamic range\n",
    "\n",
    "$$\\text{Safe Softmax}(x_i) = \\frac{e^{x_i - M}}{\\sum_{j=1}^{n} e^{x_j - M}} \\quad \\text{where } M = \\text{max} (x_i)$$\n",
    "\n",
    "\n",
    "\n",
    "**Solution**\n",
    "1. Tiling\n",
    "Matrix Mul Op have the nice property of **Associativity** & **Distributivity**. Therefore the $Q \\dot K^T$ can be tiled achieving massive paralllism on GPUs. \n",
    "2. However, the subsequent operation `SOFTMAX` CANNOT be tiled for a variety of reasons\n",
    "    * \n",
    "\n",
    "### II - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Softamx\n",
    "The online softmax (by Milakov et al) title `Online normalizer calculation for softmax` was introduced by Nvidia in 2018. \n",
    "\n",
    "1. Take care of overflows / underflow by subtracing the max value. IF the bit width is 16 (bf16, fp16).\n",
    "2. \n",
    "\n",
    "The **implication / advantage of doing Online Softmax** is that equation (7) and equation (8) can be fused since you don't require $m_{N}$ any more and only require $m_{i}$ and $m_{i-1}$. All of this is hinged on the fact that $d_{N} == d^{`}_{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q [k, :]  $  --> The Kth row of  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the space-time complexity of Attention \n",
    "* Lower Bounds on IO access\n",
    "* Parameterization\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Autograd API\n",
    "This section creates the python wrapper that integrates into Pytorch by inheriting the `torch.autograd.Function` class and implementing the `foward()` and `backward()` member functions. \n",
    "\n",
    "**FWD PASS SETUP & LAUNCH**\n",
    "\n",
    "The broad recipe we follow: \n",
    "1. Asserts: Check if the shapes of Q, K, V tensors are good. Check if values are the type and shape they should be.\n",
    "2. Allocate Input and Output buffers.\n",
    "3. Decide how to parallelize the workload - reason about grids, warps, compute units, numblocks, block size w.r.t the workload under consideration\n",
    "4. Launch FWD Triton Kernel using the `kernel_name[grid](<kernel params>)`\n",
    "5. Save CTX needed for BWD. This context would not be necessary if were doing inference. The context is essentially data that the BWD pass will require. \n",
    "- LSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Let } Q, K, V \\in \\mathbb{R}^{N \\times d} \\text{ be the query, key and value input sequences}$\n",
    "$ \\text{associated to a single head, where } N \\text{ is the sequence length and } d \\text{ is the} $\n",
    "$ \\text{head dimension. Then the attention output } O \\text{ is computed as:} $ \n",
    "\n",
    "\\begin{align*}\n",
    "& S = \\alpha QK^\\top \\in \\mathbb{R}^{N \\times N}, && \\\\ \n",
    "& P = \\text{softmax}(S) \\in \\mathbb{R}^{N \\times N}, && \\\\ \n",
    "& O = PV \\in \\mathbb{R}^{N \\times d}, &&\n",
    "\\end{align*}\n",
    "\n",
    "Then give B Batches and H heads (MHA) for each of Q, K, V we get the following shapes: \n",
    "* Q: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "* K: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "* V: `[BatchSize (B), NumHeads (H), Seq Len (N), Embedding Dim (d)]`\n",
    "\n",
    "It's key to remember that the all the batches and the heads are to be treated independently [heads don't pass around info :P ]. Therefore the effective matrix becomes `[B*H, N, d]`where `B*H` acts as the batch dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import torch \n",
    "\n",
    "def is_hip():\n",
    "    return True if triton.runtime.driver.active.get_current_target().backend == 'hip' else False\n",
    "\n",
    "\n",
    "class _attention(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, v, causal:bool, softmax_scale: float):\n",
    "        \"\"\"\n",
    "        the ctx object is used to build ctx that is required during the bwd pass\n",
    "        we'll use ctx.save_for_backward(...) to do this. \n",
    "        \"\"\"\n",
    "        ### ------ 1. Check Inputs --------------\n",
    "        HEAD_EMB_DIM_Q == HEAD_EMB_DIM_K == q.shape[-1] == k.shape[-1]  # q: [B, S, H, E]\n",
    "        HEAD_EMB_DIM_V == v.shape[-1]\n",
    "\n",
    "        assert HEAD_EMB_DIM_Q == HEAD_EMB_DIM_K == HEAD_EMB_DIM_V\n",
    "        assert HEAD_EMB_DIM_K in {16, 32, 64, 128, 256}, \"Only head size of 16, 32, 65, 128, 256 are supported\"  \n",
    "        # why are only these supported ??\n",
    "        # - usually we prefer powers of 2 because of GPU SMs (ALUs)being a power of 2 for distirbuting computation. \n",
    "        # -  \n",
    "\n",
    "        ### ------- 2. Allocate Buffers ----------\n",
    "        o = torch.empty_like(q)      # whis the output the shape of q tensor ?\n",
    "        stage = 3 if causal else 1   # causal requires special considerations such as masking. If masking is required then additional memory needs to be allocated for the boolean mask. \n",
    "        \n",
    "        ### ------ 3. Grid, Warps, EUs, Blocks etc ---------\n",
    "        # SM == EU; \n",
    "        # WARPS (32 threads) == WAVEFRONTS (64 threads) ; \n",
    "        # OCCUPANCY == WAVES_PER_EU == ACTIVE WARPS PER SM\n",
    "        # How does this affect perf\n",
    "            # - register pressure \n",
    "        if is_hip(): \n",
    "            # Increase the active warps count if ...\n",
    "            # Decrease the warp count if ...\n",
    "            waves_per_eu = 3 if HEAD_EMB_DIM_K <= 64 else 2 \n",
    "            extra_kern_args = {\"waves_per_eu\": waves_per_eu, \"allow_flush_denorm\": True}\n",
    "\n",
    "        # The grid can be 3D. \n",
    "        # Since Q.shape = [BATCH(0, B), NUM_HEADS(1, H), SEQ_LEN(2, L), EMB_DIM(3, d)] \n",
    "        # we know we can parallelize across the SEQ_LEN dim. since BATCH * NUM_HEADS is our BATCH DIM\n",
    "        x_grid = triton.cdiv(q.shape[2], BLOCK_M)  # BLOCK_M is the tile size. If seq len = 1024 and BLOCK_M=128 then NUM_BLOCKS=8. Therefore each block will process BLOCK_M elements of the seq.\n",
    "        y_grid = q.shape[0] * q.shape[1]           # BATCH * NUM_HEADS, since these are 100 % indepenedent.\n",
    "        z_grid = 1 \n",
    "        grid = (x_grid, y_grid, z_grid) # parallelization was done from inwards to outwards. \n",
    "        M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n",
    "\n",
    "        ### ------ 4. Launch ---------\n",
    "        _attn_fwd[grid](\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            softmax_scale,  # softmax scale. \n",
    "            M,  # causal mask tensor\n",
    "            o,  #\n",
    "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access. \n",
    "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access.\n",
    "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access.\n",
    "            o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #  Batch, Head, Seq, Emb strides required for correct CUDA memory access.\n",
    "            q.shape[0], q.shape[1],  #\n",
    "            N_CTX=q.shape[2],  #\n",
    "            HEAD_DIM=HEAD_DIM_K,  #\n",
    "            STAGE=stage,  #\n",
    "            **extra_kern_args\n",
    "        )\n",
    "\n",
    "        ### Save context for BWD\n",
    "        ctx.save_for_backward(q, k, v, o, M)\n",
    "        ctx.grid = grid\n",
    "        ctx.sm_scale = sm_scale\n",
    "        ctx.HEAD_DIM = HEAD_DIM_K\n",
    "        ctx.causal = causal\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward():\n",
    "        \"\"\"\n",
    "        You can make mistakes while implemetning the bwd pass. To help with this we can use the .gradcheck() to use \n",
    "        infinite small internal [f(a+h) - f(a-h)] / 2h. In addition to that people use tools like PySolver\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "attention = _attention.apply\n",
    "\n",
    "# python api\n",
    "def flash_attention_v2(batch_size: int, num_heads: int, seq_len: int, emb_dim: int, causal:bool, mode):\n",
    "    dtype = torch.float16\n",
    "    device='cuda'\n",
    "\n",
    "    q = torch.randn((batch_size, num_heads, seq_len, emb_dim), dtype=dtype, device=device) \n",
    "    k = torch.randn((batch_size, num_heads, seq_len, emb_dim), dtype=dtype, device=device) \n",
    "    v = torch.randn((batch_size, num_heads, seq_len, emb_dim), dtype=dtype, device=device) \n",
    "    \n",
    "    if mode == 'fwd':\n",
    "        q = q.to(torch.float8_e5m2) # why e5m2 for FWD pass ?\n",
    "        k = k.to(torch.float8_e5m2)  # what is _nuz ?\n",
    "\n",
    "    softmax_scale = 1.3  \n",
    "    fn = lambda: attention(q, k, v, causal, softmax_scale) # why use a lambda function here ?\n",
    "    ms = triton.testing.do_bench(fn) # benchmark the op and report time in milli seconds. \n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_HEADS = 16\n",
    "    SEQ_LEN = 4096\n",
    "    EMB_DIM = 1024\n",
    "\n",
    "    # We have many variations to support\n",
    "    # 1. QKV - Stanards MHA. Most efficient since the tensors are packed and a giant matmul can be launched.\n",
    "    # 2. QK and V: useful for GQA\n",
    "    # 3. Q and K and V\n",
    "    flash_attention_v2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton Kernel\n",
    "\n",
    "### Core Concepts\n",
    "As shown above using the **online softmax** provides us with **eventual consistency**. Therefore we can now parallelize across \n",
    "\n",
    "* X: seq length\n",
    "* Y: batch*head\n",
    "* Z: 1 [no parallelism]\n",
    "\n",
    "<mark>Each **CUDA THREAD BLOCK** gets a **UNIQUE PROGRAM ID**<mark>\n",
    "\n",
    "<img src=\"diagrams/fa_triton_program_id_mapping.png\" alt=\"Description\" width=\"700\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Memory Level         | H100                   | MI300X                 | Description                                                                                  |\n",
    "|-----------------------|-------------------------|-------------------------|----------------------------------------------------------------------------------------------|\n",
    "| Registers            | 256KB per SM, ~90 TB/s  | 256KB per CU, ~100 TB/s| Fastest memory, private to each thread. Stores variables and intermediate values for execution.|\n",
    "| Shared Memory (SMEM) | upto 258KB per SM, ~50 TB/s  | 256KB per CU, ~60 TB/s | Programmable memory local to thread blocks. Used for data sharing and reuse within a block. Nvidia combines the SMEM and L1 cache into a unified memory region that the programmer can control allocation ratio for. For applications with high thread-level data sharing (e.g., matrix multiplication), the GPU can allocate more of this 256 KB as shared memory. |\n",
    "| L1 Data Cache             | 128KB per SM, ~50 TB/s  | 128KB per CU, ~60 TB/s | Automatically caches frequently accessed local and global memory. Shared within an SM/CU.    |\n",
    "| L2 Cache             | 50MB, 37.5 TB/s         | 256MB for 8 XCDs, 50 TB/s          | Last-level cache before accessing HBM. Shared across all SMs/CUs on the GPU. Also called as AMD Infinity Cache.               |\n",
    "| HBM3                 | 80GB, 3.35 TB/s         | 192GB, 5.3 TB/s        | High Bandwidth Memory used for storing large datasets and models. Accessible by all SMs/CUs.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "BLOCK_M: Seq Tile size\n",
    "BLOCK_N\n",
    "\n",
    "\n",
    "Block level partial softmax is computed. For the partial softmax to converge to actual softmax we need to maintain 2 statistics - `m(x)`. We do this recursively over all the N/BLOCK_SIZE number of blocks,  accumulatating the results in the accumulator O.\n",
    "\n",
    "**Notatation**\n",
    "1. Given a shared memory size of $M$ we determined the block size to be  \n",
    "\n",
    "    BLOCK_SIZE = ($B_c$,   $B_r$) where $B_c = \\left\\lceil\\frac{M}{4d}\\right\\rceil$ and $B_r = \\min\\left(\\left\\lceil\\frac{M}{4d}\\right\\rceil, d\\right) $\n",
    "\n",
    "2. Next let's chunk the Q, K, and V matrices into block\n",
    "    * **row wise split**: divide the Q matrix into chunks ($Q_1, Q_2, .... Q_{Tr}$) where each chunk $Q_i$ is $B_r$ X $d$ size. We have a total of $T_r$ such chunks where $T_r = N / B_r$\n",
    "    * **col wise split**: divide the K and V matrix into chunks ($K_1, K_2, .... K_{Tc}$) where each chunk $K_i$ is $B_c$ X $d$ size. We have a total of $T_c$ such chunks where $T_c = N / B_c$\n",
    "\n",
    "3. Block wise output $O$ \n",
    "\n",
    "4. Implement **Fused Kernel** using Tiling and Online Softmax \n",
    "    * **Outer Loop**: Load $K_j$ and $V_j$ for each j in [1 .... $Tc$]\n",
    "    * **Inner Loop**: Load $Q_i$ for each i in [1 ... $Tr$]\n",
    "     \n",
    "\n",
    "<img src=\"diagrams/flash_attention_algo_v1.png\" alt=\"Description\" width=\"900\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\" style=\"vertical-align: top;  background-color: #f6f8fa; \">\n",
    "\n",
    "![Image Title](diagrams/inner_loop_attn.png)\n",
    "\n",
    "</td>\n",
    "<td width=\"55%\" style=\"vertical-align: top; background-color: #f6f8fa; padding: 10px; border-radius: 5px; font-size: 0.85em\">\n",
    "\n",
    "```python\n",
    "# Pseudocode\n",
    "for b_c in range(0, N, B_c):  # Outer loop over K, V blocks\n",
    "    # Load K,V blocks each of size (B_c x d) into SRAM \n",
    "    K_block = K[b_c:b_c+B_c, :]  # B_c tokens of the seq across all d dimension. Remember d is usually small and seq can be v large. \n",
    "    V_block = V[b_c:b_c+B_c, :]  # B_c × d block\n",
    "    \n",
    "    for b_r in range(0, N, B_r):  # Inner loop over Q blocks\n",
    "        # Load Q block into SRAM\n",
    "        Q_block = Q[b_r:b_r+B_r, :]  # B_r × d block\n",
    "        \n",
    "        # Compute attention for this block\n",
    "        S = Q_block @ K_block.T    # B_r × B_c attention scores\n",
    "        P = softmax(S)             # B_r × B_c attention weights\n",
    "        O_block = P @ V_block      # B_r × d output block\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Left: The GPU is a hierarchical computing and memory system. \n",
    "    * The HBM BW (gbps / seconds) is an order of magnitude (10x) slower that the the SMEM\n",
    "    * However, the the SMEM memory is 100x smaller than the HBM in storage (HBM: 80GB H100, 192 GB MI300x vs SMEM: )\n",
    "2. Middle: This is the core logic / kernel `_attention_fwd` of the flash attention algorithms in terms. This helps avoid the materialization of the N x N matrix \n",
    "    * The **outer for loop** loads the \n",
    "    \n",
    "<img src=\"diagrams/inner_loop_attn_paper.png\" alt=\"Description\" width=\"1000\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FWD KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attention_fwd(\n",
    "    Q,                  # Q:  M, K matrix\n",
    "    K,                  # Kt: K, N matrix\n",
    "    V,                  # V:  K, N matrix\n",
    "    softmax_scale, \n",
    "    OUT,                # O: M, N matrix\n",
    "    stride_qz, stride_qh, stride_qm, stride_qk,  # jump by stride_qz to get to next batch. \n",
    "    stride_kz, stride_kh, stride_kn, stride_kk,  # jump by stride_kk to get to next col in the Key matrix. \n",
    "    stride_vz, stride_vh, stride_vk, stride_vn,  #\n",
    "    stride_oz, stride_oh, stride_om, stride_on,  #\n",
    "    B,                  # Batch Size \n",
    "    H,                  # Number of Heads\n",
    "    N_CTX,              # \n",
    "    HEAD_DIM: tl.constexpr,  #\n",
    "    BLOCK_M: tl.constexpr,   # Tile size in the QUERY SEQ DIMENSION\n",
    "    BLOCK_N: tl.constexpr,   # Tile size for Key/Value dimension\n",
    "    STAGE: tl.constexpr      #\n",
    "):\n",
    "    \"\"\"\n",
    "    Here we try to create a highly parallelizable \"instance of the program\". Therefore we must\n",
    "    1. Define the program with the correct way for a program instance to know what data to work [which block, which tile, which batch which head & so on]\n",
    "    2. \n",
    "    \"\"\"\n",
    "    # assume [BATCH(B/Z)=4, NUM_HEADS(H)=16, SEQ_LEN(L)=512, EMBED_DIM(d)=96]\n",
    "    start_m = tl.program_id(0)    # get the current program instance along x axis. Achieve parallelism by tiling  along seq dim [X-axis]\n",
    "    offset_bh = tl.program_id(1)  # get the current program instance along y axis.  Parallelism across the Batch * Head\n",
    "\n",
    "    off_z = off_hz // H  # batch offset. if off_hz = 11 and H = 8 then batch_id = 11 // 8 = 1 (2nd batch) \n",
    "    off_h = off_hz % H   # head offset.  if off_hz = 11 and H = 8 then head_id = 11 % 8 = 3 (4th head) \n",
    "\n",
    "    # with very large tensor sizes, the offsets can be huge. Use int64 to avoid overflow. \n",
    "    # Get Q given a head and batch iie qkv_offset = batch_id * stride_batch_Q + head_id * stride_head_Q. \n",
    "    # Programs gets dispatched onto the BLOCKs(The abstraction for Triton)\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base = Q + qkv_offset,             # The base pointer to the parent tensor\n",
    "        shape = (N_CTX, HEAD_DIM),         # Shape of the parent tensor (seqlen tile size , head_dim)\n",
    "        strides = (stride_qm, stride_qk),  # The strides of the parent tensor [SeqLen X EmbedDim] ==> how to jump rows and cols of Q [mk] matrix\n",
    "        offsets = (start_m * BLOCK_M, 0)   # offsets to the block ==>  how to access the next block\n",
    "        block_shape = (BLOCK_M, HEAD_DIM)  # SEQ TILE SIZE , HEAD DIM ==>  how big is each block\n",
    "        order = (1, 0)                     # The order of the original data format\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base = K + qkv_offset,             # The base pointer to the parent tensor\n",
    "        shape = (N_CTX, HEAD_DIM),         # Shape of the parent tensor (seqlen tile size , head_dim)\n",
    "        strides = (stride_kk, stride_kn),  # The strides of the parent tensor [EmbedDim X SeqLen] ==> How to jump rows and cols of K [kn] matrix \n",
    "        offsets = (0, 0)                   # offsets to the block # why are offserts 0, 0 ?\n",
    "        block_shape = (HEAD_DIM, N_CTX)    # HEAD DIM, SEQ TILE SIZE\n",
    "        order = (0, 1)                     # The order of the original data format\n",
    "    )\n",
    "\n",
    "    v_order: tl.constexpr = (0, 1) if V.dtype.element_ty == tl.float8e5 else (1, 0)\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base = V + qkv_offset,             # The base pointer to the parent tensor\n",
    "        shape = (N_CTX, HEAD_DIM),         # Shape of the parent tensor (seqlen tile size , head_dim)\n",
    "        strides = (stride_vk, stride_vn),  # The strides of the parent tensor [EmbedDim X SeqLen] ==> How to jump rows and cols of V [kn] matrix \n",
    "        offsets=(0, 0),                    # offsets to the block\n",
    "        block_shape = (BLOCK_N, HEAD_DIM)  # SEQ TILE SIZE , HEAD DIM\n",
    "        order = v_order                    # The order of the original data format\n",
    "    )\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=OUT + qkv_offset,\n",
    "        shape=(N_CTX, HEAD_DIM),\n",
    "        strides=(stride_om, stride_on),\n",
    "        offsets=(start_m * BLOCK_M, 0),    # Output BLOCKS\n",
    "        block_shape=(BLOCK_M, HEAD_DIM), \n",
    "        order=(1, 0)\n",
    "    )\n",
    "\n",
    "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "\n",
    "    # epilogue\n",
    "\n",
    "    # Since we are dpoing everything block wise we need to have the right offset \n",
    "    # for storing m_ptrs\n",
    "    # BLOCK_SIZE + JUMP ACROSS head * num batches * partial seqlen + offset_m \n",
    "    m_ptrs = M + offset_bh * N_CTX + offs_m\n",
    "    tl.store(pointer=m_ptrs, value=m_i) # store the m_i block. \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BWD KERNEL\n",
    "\n",
    "#### PASS SETUP & LAUNCH\n",
    "1. Asserts: Check if the shapes of Q, K, V tensors are good. Check if values are the type and shape they should be\n",
    "2. Allocate Input and output buffers.\n",
    "3. Decide how to parallelize the workload: grids, warps, compute units, numblocks, block size w.r.t the workload under consideration\n",
    "4. Launch FWD Triton Kernel using the `grid[]()`\n",
    "5. Save CTX needed for BWD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Tuning\n",
    "**Tile Size**\n",
    "1. use a smaller tile size (16x16) for latency sensitive workloads. Use bigger for throughput sensitive. \n",
    "2. Smaller SMEM => Smaller tile size to fit in the fused op\n",
    "3. Higher accuracy due to accumulation over smaller ranges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "Compare across input types and sizes. \n",
    "\n",
    "1. GFLOPS per second\n",
    "2. Data movement: GBPS per second\n",
    "3. wall clock time (milliseconds)\n",
    "4. GPU Level Metrics across compute and memory hierarchy\n",
    "    * Compute\n",
    "    * Occupancy\n",
    "    * Thread Divergence\n",
    "    * Cache hit rate\n",
    "    * Register spills\n",
    "    *  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 2x3 tensor\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                      [4, 5, 6]])\n",
    "\n",
    "print(tensor.stride())  # Output: (3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.t().is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary of Terms\n",
    "1. Compute Bound vs Memory Bound\n",
    "2. Arithmetic Intensity \n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
